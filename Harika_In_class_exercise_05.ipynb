{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harika_In_class_exercise_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadhavaramHarika/Harika_INFO5731_Spring2020/blob/master/Harika_In_class_exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7TahL04sVvR",
        "colab_type": "text"
      },
      "source": [
        "# **The fifth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyZITr8sjnh",
        "colab_type": "text"
      },
      "source": [
        "## **1. Rule-based information extraction**\n",
        "\n",
        "Use any keywords related to data science, natural language processing, machine learning to search from google scholar, get the **titles** of 100 articles (either by web scraping or manually) about this topic, define a set of patterns to extract the research questions/problems, methods/algorithms/models, datasets, applications, or any other important information about this topic. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR_O9D8sOUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dcd248d5-3cbc-41d9-a8fa-190e1c25d30d"
      },
      "source": [
        "# Write your code here\n",
        "import re, numpy as nump, pandas as panda\n",
        "import urllib.request\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as panda\n",
        "import requests\n",
        "!pip install scholarly\n",
        "import scholarly\n",
        "from itertools import cycle\n",
        "import traceback\n",
        "from lxml.html import fromstring\n",
        "import string \n",
        "import nltk \n",
        "import spacy \n",
        "import numpy as nump \n",
        "import math \n",
        "from tqdm import tqdm \n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "from spacy import displacy \n",
        "\n",
        "panda.set_option('display.max_colwidth', 200)\n",
        "nlp = spacy.load(\"en_core_web_sm\")  \n",
        "\n",
        "#Retrieving proxies from URL to handle error 429-'Too many requests from the user' to google scholar url\n",
        "def get_proxies():\n",
        "    url = 'https://free-proxy-list.net/'\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    proxies = set()\n",
        "    for i in parser.xpath('//tbody/tr')[:10]:\n",
        "      if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "        #Grabbing IP and corresponding PORT\n",
        "        proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "        proxies.add(proxy)\n",
        "    return proxies\n",
        "\n",
        "#Retrieving article_items\n",
        "def getTitles(soup):\n",
        "  articles = soup.find(\"div\", {\"id\": \"gs_res_ccl_mid\"})\n",
        "  article_items = articles.find_all(class_ = \"gs_r gs_or gs_scl\")\n",
        "  list_articles = []\n",
        "  for each in article_items:\n",
        "    list_articles.append(each.find(class_ = \"gs_ri\"))\n",
        "  return list_articles\n",
        "\n",
        "#Retrieving artictle titles and storing into a list of dicts\n",
        "def getArticle_dict(articles):\n",
        "  art_list = []\n",
        "  for each in articles:\n",
        "    art_list.append({'title':each.find('h3').get_text(),'full_title': each.find(class_ = \"gs_rs\").get_text()})\n",
        "  return art_list\n",
        "  \n",
        "#Retrieving URLs for all keys\n",
        "def retrieveURLs(soup):\n",
        "  base_url = \"https://scholar.google.com/\"\n",
        "  keys = soup.find(\"div\", {\"id\": \"gs_nml\"})\n",
        "  key_urls = []\n",
        "  for each in keys.find_all('a'):\n",
        "    if int(each.get_text())<11:\n",
        "      key_urls.append(str(base_url+each.get('href')))\n",
        "  return key_urls\n",
        "\n",
        "#Retrieving URL data using beautifulsoup\n",
        "def getDataFromUrl(url):\n",
        "  proxies = get_proxies()\n",
        "  proxy_pool = cycle(proxies)\n",
        "  proxy = next(proxy_pool)\n",
        "  try:\n",
        "    response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy}).text\n",
        "    b_soup = BeautifulSoup(response,'html.parser')\n",
        "    return b_soup   \n",
        "  except:\n",
        "    #Skipping connection errors as all above created proxies won't connect to the url and gives connection issues,\n",
        "    print(\"Skipping. Connnection error\")\n",
        "\n",
        "#Taking POS Tags\n",
        "def getPOSforText(text):\n",
        "  doc = nlp(text)\n",
        "  for token in doc: \n",
        "    print(token.text, \"-->\",token.dep_,\"-->\", token.pos_)\n",
        "  return doc\n",
        "\n",
        "#describing question Pattern\n",
        "def questPattern(token_doc):\n",
        "  pattern = [{'POS':'PROPN'}, \n",
        "             {'POS':'advmod'},\n",
        "           {'LOWER': 'is'}, \n",
        "           {'LOWER':'are'},\n",
        "           {'LOWER': 'was'}, \n",
        "           {'POS': 'NOUN','OP':'?'},\n",
        "           {'POS':'amod','OP':'?'}]\n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern) \n",
        "  matches = matcher(token_doc) \n",
        "  span = token_doc[matches[0][1]:] \n",
        "  return span.text\n",
        "\n",
        "#describing Problem Pattern\n",
        "def probPattern(token_doc):\n",
        "  pattern = [{'POS':'PROPN'}, \n",
        "             {'POS':'NOUN'},\n",
        "           {'LOWER': 'in'}, \n",
        "           {'LOWER':'of'},\n",
        "           {'LOWER': 'are'},\n",
        "            {'LOWER':'were'}, \n",
        "           {'POS': 'NOUN','OP':'?'},\n",
        "           {'POS':'amod','OP':'?'}]\n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern) \n",
        "  matches = matcher(token_doc) \n",
        "  span = token_doc[matches[0][1]:] \n",
        "  return span.text\n",
        "\n",
        "#describing fact Pattern\n",
        "def factPattern(token_doc):\n",
        "  pattern = [{'POS':'PROPN'}, \n",
        "             {'POS':'NOUN'},\n",
        "           {'LOWER': 'as'}, \n",
        "           {'LOWER':'a'},\n",
        "           {'LOWER': 'is'},\n",
        "            {'LOWER':'in'}, \n",
        "             {'LOWER':'of'},\n",
        "           {'POS': 'NOUN','OP':','},\n",
        "           {'POS':'VERB'}]\n",
        "\n",
        "  matcher = Matcher(nlp.vocab) \n",
        "  matcher.add(\"matching_1\", None, pattern) \n",
        "  matches = matcher(token_doc) \n",
        "  span = token_doc[matches[0][1]:] \n",
        "  return span.text\n",
        "\n",
        "#Constructing url to search article in google Scholr with keywords-Pattern Recognition Machine Learning \n",
        "query = \"pattern+recognition+machine+learning\"\n",
        "url = 'https://scholar.google.com/scholar?q=' + query + '&ie=UTF-8&oe=UTF-8&hl=en&btnG=Search'\n",
        "\n",
        "#Retrieving all the article titles from above functions\n",
        "content = getDataFromUrl(url)\n",
        "print(content)\n",
        "if content:\n",
        "  allTitles = getTitles(content)\n",
        "  print(len(allTitles))\n",
        "  '''if content:\n",
        "    for each in retrieveURLs(content):\n",
        "      allTitles.append(getTitles(getDataFromUrl(each)))\n",
        "      print(len(allTitles))'''\n",
        "\n",
        "#Calling above function to print the patterned texts from all the articles\n",
        "for each in getArticle_dict(allTitles):\n",
        "  print(questPattern(getPOSforText(each['full_title'])))\n",
        "  print(probPattern(getPOSforText(each['full_title'])))\n",
        "  print(factPattern(getPOSforText(each['full_title'])))\n",
        "\n",
        "\n"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from scholarly) (4.6.3)\n",
            "Requirement already satisfied: requests[security] in /usr/local/lib/python3.6/dist-packages (from scholarly) (2.22.0)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.6/dist-packages (from scholarly) (1.1.0)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.6/dist-packages (from scholarly) (0.15.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[security]->scholarly) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[security]->scholarly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[security]->scholarly) (1.24.3)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Skipping. Connnection error\n",
            "None\n",
            "This --> nsubj --> DET\n",
            "is --> ROOT --> VERB\n",
            "the --> det --> DET\n",
            "first --> amod --> ADJ\n",
            "textbook --> attr --> NOUN\n",
            "on --> prep --> ADP\n",
            "pattern --> compound --> NOUN\n",
            "recognition --> pobj --> NOUN\n",
            "to --> aux --> PART\n",
            "present --> relcl --> VERB\n",
            "the --> det --> DET\n",
            "Bayesian --> amod --> ADJ\n",
            "viewpoint --> dobj --> NOUN\n",
            ". --> punct --> PUNCT\n",
            "The --> det --> DET\n",
            "book --> compound --> NOUN\n",
            "presents --> nsubj --> VERB\n",
            "approximate --> ROOT --> ADJ\n",
            "inference --> compound --> NOUN\n",
            "algorithms --> dobj --> NOUN\n",
            "that --> nsubj --> DET\n",
            "permit --> relcl --> VERB\n",
            "fast --> amod --> ADJ\n",
            "approximate --> amod --> ADJ\n",
            "answers --> dobj --> NOUN\n",
            "in --> prep --> ADP\n",
            "situations --> pobj --> NOUN\n",
            "where --> advmod --> ADV\n",
            "exact --> amod --> ADJ\n",
            "answers --> nsubj --> NOUN\n",
            "are --> relcl --> VERB\n",
            "not --> neg --> ADV\n",
            "feasible --> acomp --> ADJ\n",
            ". --> punct --> PUNCT\n",
            "It --> nsubj --> PRON\n",
            "uses --> ROOT --> VERB\n",
            "graphical --> amod --> ADJ\n",
            "models --> dobj --> NOUN\n",
            "to --> aux --> PART\n",
            "describe --> xcomp --> VERB\n",
            "  -->  --> SPACE\n",
            "… --> dobj --> X\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-8a936d1a9472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m#Calling above function to print the patterned texts from all the articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetArticle_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallTitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestPattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetPOSforText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobPattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetPOSforText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactPattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetPOSforText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-170-8a936d1a9472>\u001b[0m in \u001b[0;36mquestPattern\u001b[0;34m(token_doc)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matching_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq_7VGmrsum4",
        "colab_type": "text"
      },
      "source": [
        "## **2. Domain-specific information extraction**\n",
        "\n",
        "For the legal case used in the data cleaning exercise: [01-05-1 Adams v Tanner.txt](https://raw.githubusercontent.com/unt-iialab/INFO5731_Spring2020/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt), use [legalNLP](https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods) to extract the following inforation from the text (if the information is not exist, just print None):\n",
        "\n",
        "(1) acts, e.g., “section 1 of the Advancing Hope Act, 1986”\n",
        "\n",
        "(2) amounts, e.g., “ten pounds” or “5.8 megawatts”\n",
        "\n",
        "(3) citations, e.g., “10 U.S. 100” or “1998 S. Ct. 1”\n",
        "\n",
        "(4) companies, e.g., “Lexpredict LLC”\n",
        "\n",
        "(5) conditions, e.g., “subject to …” or “unless and until …”\n",
        "\n",
        "(6) constraints, e.g., “no more than”\n",
        "\n",
        "(7) copyright, e.g., “(C) Copyright 2000 Acme”\n",
        "\n",
        "(8) courts, e.g., “Supreme Court of New York”\n",
        "\n",
        "(9) CUSIP, e.g., “392690QT3”\n",
        "\n",
        "(10) dates, e.g., “June 1, 2017” or “2018-01-01”\n",
        "\n",
        "(11) definitions, e.g., “Term shall mean …”\n",
        "\n",
        "(12) distances, e.g., “fifteen miles”\n",
        "\n",
        "(13) durations, e.g., “ten years” or “thirty days”\n",
        "\n",
        "(14) geographic and geopolitical entities, e.g., “New York” or “Norway”\n",
        "\n",
        "(15) money and currency usages, e.g., “$5” or “10 Euro”\n",
        "\n",
        "(16) percents and rates, e.g., “10%” or “50 bps”\n",
        "\n",
        "(17) PII, e.g., “212-212-2121” or “999-999-9999”\n",
        "\n",
        "(18) ratios, e.g.,” 3:1” or “four to three”\n",
        "\n",
        "(19) regulations, e.g., “32 CFR 170”\n",
        "\n",
        "(20) trademarks, e.g., “MyApp (TM)”\n",
        "\n",
        "(21) URLs, e.g., “http://acme.com/”\n",
        "\n",
        "(22) addresses, e.g., “1999 Mount Read Blvd, Rochester, NY, USA, 14615”\n",
        "\n",
        "(23) persons, e.g., “John Doe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc7NtJrLx5tS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e23515a3-c0cc-4a94-d486-0543125a1a36"
      },
      "source": [
        "# write your code here\n",
        "import nltk\n",
        "import csv,os,math,unicodedata,re\n",
        "from google.colab import drive,files\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "import pandas as panda\n",
        "!pip install lexnlp\n",
        "import lexnlp.extract.en.acts as act\n",
        "import lexnlp.extract.en.citations as cite\n",
        "import lexnlp.extract.en.amounts as amount\n",
        "import lexnlp.extract.en.conditions as cond\n",
        "import lexnlp.extract.en.constraints as const\n",
        "import lexnlp.extract.en.copyright as copy\n",
        "import lexnlp.extract.en.courts as court\n",
        "import lexnlp.extract.en.cusip\n",
        "import lexnlp.extract.en.dates\n",
        "import lexnlp.extract.en.definitions\n",
        "import lexnlp.extract.en.distances\n",
        "import lexnlp.extract.en.durations\n",
        "import lexnlp.extract.en.geoentities\n",
        "import lexnlp.extract.en.money\n",
        "import lexnlp.extract.en.percents\n",
        "import lexnlp.extract.en.pii\n",
        "import lexnlp.extract.en.ratios\n",
        "import lexnlp.extract.en.trademarks\n",
        "import lexnlp.extract.en.urls\n",
        "import lexnlp.extract.en.regulations\n",
        "import lexnlp.extract.en.entities.nltk_re\n",
        "import lexnlp.extract as extract\n",
        "\n",
        "#Opening Legal_text file from google drive\n",
        "def open_textfile():\n",
        "  drive.mount('/content/gdrive')\n",
        "  file_list = os.listdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "  os.chdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "  if 'Legal_case.txt' in file_list:\n",
        "    text_data = open('Legal_case.txt').read()\n",
        "  clean_data = text_data.replace('\\xa0','')\n",
        "  return clean_data\n",
        "\n",
        "#Splitting document into text sentences and removing non-alphanumeric characters\n",
        "def getTextSentences(samp_data):\n",
        "  text_sent = [re.sub(r\"\\W\", \" \", each, flags=re.I) for each in samp_data.split('\\n')\n",
        "                  if each]\n",
        "  return text_sent\n",
        "\n",
        "#Extracting Acts from each text sentence\n",
        "def extractActs(textData):\n",
        "  acts_list = []\n",
        "  for each in textData:\n",
        "    temp = list(act.get_act_list(each))\n",
        "    if len(temp)!=0:\n",
        "      acts_list.append(temp)\n",
        "  return acts_list if acts_list else None\n",
        "\n",
        "#Extracting Amounts from each text sentence\n",
        "def extractAmounts(textData):\n",
        "  amnt_list = []\n",
        "  for each in textData:\n",
        "    temp = list(amount.get_amounts(each))\n",
        "    if len(temp)!=0:\n",
        "     amnt_list.append(temp)\n",
        "  return amnt_list if amnt_list else None\n",
        "\n",
        "#Extracting Citations from each text sentence\n",
        "def extractCitation(textData):\n",
        "  cite_list = []\n",
        "  for each in textData:\n",
        "    temp = list(cite.get_citations(each))\n",
        "    if len(temp)!=0:\n",
        "     cite_list.append(temp)\n",
        "  return cite_list if cite_list else None \n",
        "\n",
        "#Extracting Companies from each text sentence\n",
        "def extractCompanies(textData):\n",
        "  comp_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.entities.nltk_re.get_companies(each))\n",
        "    if len(temp)!=0:\n",
        "     comp_list.append(temp)\n",
        "  return comp_list if comp_list else None \n",
        "\n",
        "#Extracting Conditions from each text sentence\n",
        "def extractConditions(textData):\n",
        "  cond_list = []\n",
        "  for each in textData:\n",
        "    temp = list(cond.get_conditions(each))\n",
        "    if len(temp)!=0:\n",
        "     cond_list.append(temp)\n",
        "  return cond_list if cond_list else None \n",
        "\n",
        "#Extracting Constraints from each text sentence\n",
        "def extractConstraints(textData):\n",
        "  const_list = []\n",
        "  for each in textData:\n",
        "    temp = list(const.get_constraints(each))\n",
        "    if len(temp)!=0:\n",
        "     const_list.append(temp)\n",
        "  return const_list if const_list else None \n",
        "\n",
        "#Extracting Copyrights from each text sentence\n",
        "def extractCopyrights(textData):\n",
        "  copy_list = []\n",
        "  for each in textData:\n",
        "    temp = list(copy.get_copyright(each))\n",
        "    if len(temp)!=0:\n",
        "     copy_list.append(temp)\n",
        "  return copy_list if copy_list else None\n",
        "\n",
        "#Extracting CUSIP from each text sentence\n",
        "def extractCUSIP(textData):\n",
        "  cusip_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.cusip.get_cusip(each))\n",
        "    if len(temp)!=0:\n",
        "     cusip_list.append(temp)\n",
        "  return cusip_list if cusip_list else None\n",
        "\n",
        "#Extracting Dates from each text sentence\n",
        "def extractDates(textData):\n",
        "  date_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.dates.get_dates(each))\n",
        "    if len(temp)!=0:\n",
        "     date_list.append(temp)\n",
        "  return date_list if date_list else None\n",
        "\n",
        "#Extracting Definitions from each text sentence\n",
        "def extractDefinitions(textData):\n",
        "  def_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.definitions.get_definitions(each))\n",
        "    if len(temp)!=0:\n",
        "     def_list.append(temp)\n",
        "  return def_list if def_list else None  \n",
        "\n",
        "#Extracting Distances from each text sentence\n",
        "def extractDistances(textData):\n",
        "  dist_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.distances.get_distances(each))\n",
        "    if len(temp)!=0:\n",
        "     dist_list.append(temp)\n",
        "  return dist_list if dist_list else None\n",
        "\n",
        "#Extracting Durations from each text sentence\n",
        "def extractDurations(textData):\n",
        "  dur_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.durations.get_durations(each))\n",
        "    if len(temp)!=0:\n",
        "     dur_list.append(temp)\n",
        "  return dur_list if dur_list else None\n",
        "\n",
        "#Extracting courts from each text sentence\n",
        "def extractCourts(textData):\n",
        "  file_data = panda.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/master/en/legal/us_courts.csv\",error_bad_lines=False)\n",
        "  court_config = []\n",
        "  courts_list = []\n",
        "  for i in range(len(file_data)):\n",
        "    a = str(file_data[\"Alias\"][i]).split(\";\")\n",
        "    alias_list = [(each,None,False,a.index(each)) for each in a]\n",
        "    c = (file_data[\"Court ID\"][i],file_data[\"Court Name\"][i],0,alias_list)\n",
        "    court_config.append(c)\n",
        "  for each in textData:\n",
        "    temp = {}\n",
        "    for entity, alias in lexnlp.extract.en.courts.get_courts(each,court_config):\n",
        "      temp[\"enity\"] = entity\n",
        "      temp[\"alias\"] = alias\n",
        "    if temp:\n",
        "      courts_list.append(temp)\n",
        "  return courts_list if courts_list else None\n",
        "\n",
        "#Extracting Geographics and geopolitics from each text sentence\n",
        "def extractGeoPolitics(textData):\n",
        "  file_data = panda.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/master/en/geopolitical/us_state_regulators.csv\",error_bad_lines=False)\n",
        "  geo_config = []\n",
        "  geo_list = []\n",
        "  for i in range(len(file_data)):\n",
        "    a = str(file_data[\"Alias\"][i]).split(\";\")\n",
        "    alias_list = [(each,None,file_data[\"Case Sensitive\"][i],a.index(each)) for each in a]\n",
        "    g = (i,file_data[\"Term\"][i],0,alias_list)\n",
        "    geo_config.append(g)\n",
        "  for each in textData:\n",
        "    temp = {}\n",
        "    for entity, alias in lexnlp.extract.en.geoentities.get_geoentities(each,geo_config):\n",
        "      temp[\"enity\"] = entity\n",
        "      temp[\"alias\"] = alias\n",
        "    if temp:\n",
        "      geo_list.append(temp)\n",
        "  return geo_list if geo_list else None\n",
        "\n",
        "#Extracting Money and Currency from each text sentence\n",
        "def extractMoneyCurrency(textData):\n",
        "  money_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.money.get_money(each))\n",
        "    if len(temp)!=0:\n",
        "     money_list.append(temp)\n",
        "  return money_list if money_list else None\n",
        "\n",
        "#Extracting Percent and Rates from each text sentence\n",
        "def extractPercentRates(textData):\n",
        "  percent_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.percents.get_percents(each))\n",
        "    if len(temp)!=0:\n",
        "     percent_list.append(temp)\n",
        "  return percent_list if percent_list else None\n",
        "\n",
        "#Extracting PII from each text sentence\n",
        "def extractPII(textData):\n",
        "  pii_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.pii.get_pii(each))\n",
        "    if len(temp)!=0:\n",
        "     pii_list.append(temp)\n",
        "  return pii_list if pii_list else None\n",
        "\n",
        "#Extracting Ratios from each text sentence\n",
        "def extractRatios(textData):\n",
        "  ratio_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.ratios.get_ratios(each))\n",
        "    if len(temp)!=0:\n",
        "     ratio_list.append(temp)\n",
        "  return ratio_list if ratio_list else None\n",
        "\n",
        "#Extracting Regulations from each text sentence\n",
        "def extractRegulations(textData):\n",
        "  reg_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.regulations.get_regulations(each))\n",
        "    if len(temp)!=0:\n",
        "     reg_list.append(temp)\n",
        "  return reg_list if reg_list else None\n",
        "\n",
        "#Extracting Trademarks from each text sentence\n",
        "def extractTrademarks(textData):\n",
        "  trad_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.trademarks.get_trademarks(each))\n",
        "    if len(temp)!=0:\n",
        "     trad_list.append(temp)\n",
        "  return trad_list if trad_list else None\n",
        "\n",
        "#Extracting URLs from each text sentence\n",
        "def extractURLs(textData):\n",
        "  url_list = []\n",
        "  for each in textData:\n",
        "    temp = list(lexnlp.extract.en.urls.get_urls(each))\n",
        "    if len(temp)!=0:\n",
        "     url_list.append(temp)\n",
        "  return url_list if url_list else None\n",
        "\n",
        "#Extracting pos_tags from each text sentence\n",
        "def basicFeatExtractio(TextSent):\n",
        "  stop = stopwords.words('english')\n",
        "  sent = ' '.join([w for w in TextSent.split() if w not in stop])\n",
        "  tokens = nltk.word_tokenize(sent)\n",
        "  pos_words = nltk.pos_tag(tokens)\n",
        "  return pos_words\n",
        "\n",
        "#Extracting Address and Location from each text sentence\n",
        "def extractNamesLocations(textData):\n",
        "  names = []\n",
        "  address = []\n",
        "  for each in textData:\n",
        "    name_sent = []\n",
        "    address_sent = []\n",
        "    tag_sent = basicFeatExtractio(each)\n",
        "    for chunk in nltk.ne_chunk(tag_sent):\n",
        "      if type(chunk) == nltk.tree.Tree:\n",
        "          if chunk.label() == 'PERSON':\n",
        "            name_sent.append(' '.join([w for w,t in chunk]))\n",
        "          elif chunk.label == 'LOCATION':\n",
        "            address_sent.append(' '.join([w for w,t in chunk]))\n",
        "    if name_sent:\n",
        "      names.append(name_sent)\n",
        "    if address_sent:\n",
        "      address.append(address_sent)\n",
        "  return (names if names else None,address if address else None)\n",
        "\n",
        "#Calling above functions to print results\n",
        "text_data = open_textfile()\n",
        "text_sent = getTextSentences(text_data)\n",
        "\n",
        "print(\"\\nActs :\\n \")\n",
        "print(extractActs(text_sent))\n",
        "\n",
        "print(\"\\nAmounts :\\n\")\n",
        "print(extractAmounts(text_sent))\n",
        "\n",
        "print(\"\\nCitations :\\n\")\n",
        "print(extractCitation(text_sent))\n",
        "\n",
        "print(\"\\nCompanies :\\n\")\n",
        "print(extractCompanies(text_sent))\n",
        "\n",
        "print(\"\\nConditions :\\n\")\n",
        "print(extractConditions(text_sent))\n",
        "\n",
        "print(\"\\nConstraints :\\n\")\n",
        "print(extractConstraints(text_sent))\n",
        "\n",
        "print(\"\\nCopyrights :\\n\")\n",
        "print(extractCopyrights(text_sent))\n",
        "\n",
        "print(\"\\nCourts :\\n\")\n",
        "print(extractCourts(text_sent))\n",
        "\n",
        "print(\"\\nCUSIP :\\n\")\n",
        "print(extractCUSIP(text_sent))\n",
        "\n",
        "print(\"\\nDates :\\n\")\n",
        "print(extractDates(text_sent))\n",
        "\n",
        "print(\"\\nDefinitions :\\n\")\n",
        "print(extractDefinitions(text_sent))\n",
        "\n",
        "print(\"\\nDistances :\\n\")\n",
        "print(extractDistances(text_sent))\n",
        "\n",
        "print(\"\\nDurations :\\n\")\n",
        "print(extractDurations(text_sent))\n",
        "\n",
        "print(\"\\nGeographics And Geopolitics:\\n\")\n",
        "print(extractGeoPolitics(text_sent))\n",
        "\n",
        "print(\"\\nMoney And currency :\\n\")\n",
        "print(extractMoneyCurrency(text_sent))\n",
        "\n",
        "print(\"\\nPercent And Rates :\\n\")\n",
        "print(extractPercentRates(text_sent))\n",
        "\n",
        "print(\"\\nPII :\\n\")\n",
        "print(extractPII(text_sent))\n",
        "\n",
        "print(\"\\nRatios :\\n\")\n",
        "print(extractRatios(text_sent))\n",
        "\n",
        "print(\"\\nRegulstions :\\n\")\n",
        "print(extractRegulations(text_sent))\n",
        "\n",
        "print(\"\\nTrademarks :\\n\")\n",
        "print(extractTrademarks(text_sent))\n",
        "\n",
        "print(\"\\nURLs :\\n\")\n",
        "print(extractURLs(text_sent))\n",
        "\n",
        "results = extractNamesLocations(text_sent)\n",
        "print(\"\\nNames: \\n\",results[0])\n",
        "print(\"\\nAddress: \\n\",results[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "Requirement already satisfied: lexnlp in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2.22.0)\n",
            "Requirement already satisfied: scipy==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: pycountry==18.5.26 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (18.5.26)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.5)\n",
            "Requirement already satisfied: gensim==3.4.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.4.0)\n",
            "Requirement already satisfied: Unidecode==0.4.21 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.4.21)\n",
            "Requirement already satisfied: typing==3.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (3.6.2)\n",
            "Requirement already satisfied: datefinder-lexpredict==0.6.2 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.6.2)\n",
            "Requirement already satisfied: reporters-db==1.0.12.1 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.12.1)\n",
            "Requirement already satisfied: regex==2017.9.23 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (2017.9.23)\n",
            "Requirement already satisfied: pandas==0.23.4 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.23.4)\n",
            "Requirement already satisfied: us==1.0.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (1.0.0)\n",
            "Requirement already satisfied: dateparser==0.7.0 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.7.0)\n",
            "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.21.3)\n",
            "Requirement already satisfied: num2words==0.5.7 in /usr/local/lib/python3.6/dist-packages (from lexnlp) (0.5.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2.8)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.0.0->lexnlp) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->lexnlp) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from datefinder-lexpredict==0.6.2->lexnlp) (2.6.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datefinder-lexpredict==0.6.2->lexnlp) (2018.9)\n",
            "Requirement already satisfied: jellyfish==0.5.6 in /usr/local/lib/python3.6/dist-packages (from us==1.0.0->lexnlp) (0.5.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->lexnlp) (0.14.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.14.15)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.15.2)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\n",
            "Acts :\n",
            " \n",
            "None\n",
            "\n",
            "Amounts :\n",
            "\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "Warning: parsing empty text\n",
            "[[5.0, 740.0], [1843.0], [2.0], [1.0], [4.0], [2.0], [1821.0], [5.0], [1.0, 1840.0, 3777, 80.0, 100.0, 30, 1839.0, 741.0, 22, 1840.0, 14000, 120, 1, 1840.0, 3, 4], [1], [1.0, 1840.0, 2.0, 1.0, 361.0, 1.0, 307.0, 6.0, 604.0, 1.0, 2.0, 418.0, 422.0, 7.0, 34.0, 41.0, 167.0, 742.0, 3.0, 112.0, 207.0, 3.0, 338.0, 424.0, 5.0, 26.0, 13.0, 235.0, 8.0, 693.0, 4.0], [1821.0, 167.0], [2.0, 2.0, 216.0, 3.0, 66.0, 4.0, 130.0], [29.0, 2.0, 241.0, 2.0, 332.0, 2.0, 422.0, 9.0, 112.0, 743.0, 9.0, 39.0, 14000], [1840.0], [744.0, 5.0, 182.0], [3.0, 368.0, 1.0, 397.0, 6.0, 604.0, 1, 1821.0, 167.0, 745.0], [4.0], [746.0], [4.0, 210.0, 46.0], [747.0], [5.0], [5.0, 740.0, 1843.0, 284.0], [2019.0], [9.0], [1.0], [55.0, 266.0, 271.0], [1876.0], [2.0], [47.0, 362.0, 376.0], [1872.0], [3.0], [45.0, 329.0, 334.0], [1871.0], [4.0], [31.0, 526.0, 527.0], [1858.0], [5.0], [21.0, 333.0, 335.0], [1852.0], [6.0], [8.0, 145.0, 147.0], [1857.0], [7.0], [65.0, 256.0, 258.0], [3], [1880.0], [8.0], [4.0, 913.0, 914.0], [1887.0], [9.0], [103.0, 464.0], [1936.0], [3.0], [1.0], [9.0, 39.0, 1828.0], [2.0], [2.0], [5.0, 182.0, 1837.0], [2.0], [3.0], [9.0, 108.0, 1812.0], [6, 1], [2.0]]\n",
            "\n",
            "Citations :\n",
            "\n",
            "[[(1843, 'WL', 'West Law Citation', 284, None, None, None)]]\n",
            "\n",
            "Companies :\n",
            "\n",
            "[[Lehman  Durr Co, (4, 22)]]\n",
            "\n",
            "Conditions :\n",
            "\n",
            "[[('until', 'Under St 1821  prohibiting a levy on a crop', ''), ('until', 'it has been gathered  no lien attaches in favor of a fi  fa  on a growing crop  nor does such lien attach', '')], [('if', '1 This was a trial of the right of property under the statute  In November  1840  an execution issued from the circuit court of Sumter  at the suit of the plaintiff in error  requiring the sheriff of that county  to make of the goods   c   of Allen Harrison and others  the sum of thirty seven hundred and seventy seven 80 100 dollars  besides costs  This execution was levied on thirty bales of cotton  as the property of Allen Harrison  which was claimed  and a bond given to try the right  An issue was made up to try the question of the liability of the cotton to the plaintiff s execution  and submitted to a jury  On the trial  a bill of exceptions was sealed at the instance of the plaintiff  The plaintiff proved that he recovered his judgment in October  1839  that an execution  741 was issued thereon on the 7th Nov  thereafter  and that alias and pluries fieri facias   issued regularly up to the time levy was made  that the cotton levied on was growed on the plantation of Harrison  and cultivated by the hands in his service  It was proved by the claimants  by the production of a written contract  that Harrison  on the twenty second of May  1840  in consideration that the claimants were involved  as indorsers for Burton   Harrison of Sumter county  and were then exposed to an execution  amounting to upwards of fourteen thousand dollars  bargained and sold to the claimants all his growing crop of cotton  c   consisting of one hundred and twenty acres   c  Allen Harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers ', ''), ('where', 'the crop matured and was gathered he undertook to deliver the cotton in Gainesville  The claimants came from Tennessee  ', ''), ('when', 'they resided  about the first of September  1840  bringing with them three or four white laborers  and took possession of the crop and slaves  and with the latter  and white laborers  gathered the cotton  prepared it for market  and', '')], [('if', 'The court charged the jury  that the plaintiff had no lien by virtue of his judgment  and execution on the growing crop  that Harrison had a right to convey it  without being in any manner restrained by them  that the writing adduced  was a sale of the crop  but', ''), ('when', 'it was not  and the lien of the fieri facias would have attached upon it ', ''), ('if', 'gathered  yet', ''), ('not subject to', 'the claimants obtained possession on the first of September  and controlled the gathering of the crop  then no lien attached  and it was', '')], [('until', 'R  H  SMITH  for the plaintiff  in error  made the following points   1  The crop of Harrison  must  in May  1840  have been in an immature state  and it is insisted  was not the subject of a sale  2  By the common law  a growing crop could be levied on and sold   1 Salk  Rep  361  1 Bos    P  Rep  307  6 East s Rep  604  note 1  2 Johns  Rep  418  422  7 Mass  Rep  34   and our statue     Aik  Dig    41  p  167   which forbids the levy of an  742 execution on a growing crop  is to receive a strict construction  It merely inhibits the levy  but the lien attaches  and a levy and sale may be made after the crop matures  and is gathered  3  The contract does not purport to convey the growing crop  but is a mere executory agreement  requiring some act to be done by Harrison  in order to invest the claimants with the right of property   Chit  on Con  112  207  3 Johns  Rep  338  424  5 Wend  Rep  26  13 Johns  Rep  235  8 Dowl  Rep  693   and', '')], [('until', 'W  M  MURPHY  with whom was W  G  JONES  for the defendant  cited the act of 1821   Aik  Dig  167   which declares it to be lawful to levy an execution on a planted crop ', '')], [('if', 'It is admitted that the contract between the defendant in execution  and the claimants  was in good faith ', '')], [('when', 'There can be no doubt that a growing crop has such an existence as to be the subject matter of a sale  mortgage  or other contract which possess an interest to vest in possession  either immediately or at some future time  This proposition has frequently been assumed as unquestionable  the point of inquiry generally being  whether under a statute of frauds  such as the 29 Chas  2  it is a mere chattel  and transferrable by parol without writing   Chitty on Con  241 2  332  Whipple v  Foot  2 Johns  Rep  422  Stewart v  Doughty  9 Johns  Rep  112   743 Austin v  Sawyer  9 Cow  Rep  39  See also Ravesies v  Lee   Alston  at last term   The contract set out in the bill of exceptions  we are inclined to think evidences rather a mortgage than an absolute sale  It recites that the claimants are involved as indorsers of a mercantile firm  of which the defendant was a partner  that an execution for upwards of fourteen thousand dollars against their estate  is in the sheriff s hands  and that a conveyance is made of the crop of cotton  corn and oats  which the grantor agrees to give up at any time to the use of the claimants  so as to prevent injury to them as indorsers  The defendant in execution might at any time have divested the interest which the contract vested in the claimants  by discharging their liability as his indorsers  or a judgment creditor might have satisfied the lien  and', '')], [('unless', 'We will then consider the writing under which the claimants assert a right  as a mortgage with a power to take possession any time during the year ', '')], [('if', 'The claimants had previous to the levy of the execution taken possession of the crop  prepared the cotton for market  and removed it to a ware house  This possession  it is insisted  was a trespass  because it was acquired in the absence of the defendant in execution  and without his consent then given  Conceding the truth of the facts stated in the bill of exceptions  and we think it will not follow  that the possession of the claimants is a nullity  and that the case must be considered as', ''), ('if', 'they had never interfered with the crop  The contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity  and', ''), ('if', 'they took possession of it in the absence of the grantor   though without his consent  ', ''), ('if', 'he subsequently acquiesced in it  the inference would be ', '')], [('subject to', '3 This brings us back to the question  whether the execution of the plaintiff was a lien on the growing crop  so as to defeat the mortgage to the claimants  It has been frequently mooted whether  at common law  corn   c   before it is gathered  can be seized under a fieri facias  Mr  Dane  in remarking upon this point  says   The American editor of Bacon s Abridgment  says   Wheat growing in the ground is a chattel  and', ''), ('until', 'be taken in execution  and the sheriff may suffer it to grow till harvest  and then cut and sell it  or may perhaps sell it growing  and the purchaser will then be entitled to enter  for the purpose of cutting and carrying it away    He cites Whipple v  Foot  ut supra  also Poole s case  Salk  368  1 Bos    P  397  6 East  604  n   But Whipple v  Foot seems to be the only case that supports his position  that unripe wheat or corn may be taken in execution  and the same editor states that nothing can be taken in execution which cannot be sold  This position  says the learned commentator  is no doubt law  But it is unnecessary to consider how this matter stands at common law  The first section of the act of 1821   To prevent sheriffs and other officers from levying executions in certain cases  enacts  that  It shall not be lawful for any sheriff or other officer  to levy a writ of fieri facias or other execution on the planted crop of a debtor  or person against whom an execution may issue ', ''), ('until', 'the crop is gathered    Aik  Dig  167   Now here is an express inhibition to levy an execution on a crop while it remains on  or in the ground  and', ''), ('if', 'it is severed from the soil to which it owes its growth  In respect to property thus situated  will the lien of an execution attach eo instanti upon its being placed in the hands of an officer ', ''), ('until', 'so  the act cited  will only have the effect of keeping the right to levy it in abeyance', ''), ('if', 'the crop is gathered  The lien of an execution does  not only operate so as to prevent the debtor from disposing of the property on which it attaches  but gives to the creditor the right to have it sold to satisfy his  745 judgment  The lien and the right to levy are intimately connected  and', ''), ('until', 'the latter be taken away  or suspended  the effect  at common law  is the destruction of the former  This principle is fully established by Mansony and Hurtell v  The President   c  of the Bank of the United States  and its assignees  and the citations contained in the opinion of the court in that case  as also in my opinion in Wood v  Gary  et al   both decided at the last term  That it was competent for the legislature to have made it unlawful to levy an execution on particular property ', ''), ('if', 'its condition was changed  and still to give it a continuing lien  cannot be doubted  but there is nothing in the act in question to indicate that such is its intention ', ''), ('until', 'the object was merely to suspend the sale ', ''), ('as soon as', 'the crop was gathered  it would have been very easy to have said so in explicit terms  but declaring as the statute does  in totidem verbis  that the execution shall not be levied  the legislature must be supposed to have meant what they have expressed  The act was induced by the doubts which existed as to what was the common law  and was intended to remove those doubts by declaring what should be the law in future  It does not create a lien or authorize a levy in a case in which the law  as it then existed  was silent  The idea that the lien attached upon the planted crop', ''), ('until', 'the execution was delivered to the sheriff  though the right to levy it was postponed', ''), ('until', 'a severance took place  is attempted to be deduced from the last words of the section cited  viz  ', ''), ('if', 'the crop is gathered   These words cannot  upon any just principles of construction be regarded so potent as to give to an execution a retrospective effect  They do not refer to the lien ', ''), ('until', 'they did they would postpone it', ''), ('until', 'the crop was gathered  but it is the levy they relate to and postpone', '')], [('until', '4 The right to levy an execution on a planted crop  then  being expressly taken away by the statute  the lien which is connected with and consequent upon that right  never attaches', '')], [('if', 'The circuit judge may have mistaken the law in supposing that the contract was a sale  but', ''), ('when', 'he did  an error in that respect was very immaterial  for whether a sale or mortgage  as we have  746 seen  under the facts of the case  the defendant in execution has no interest that could be seised and sold under execution  There is no assumption of any material fact in the charge  but the possession of the claimant  the time', ''), ('if', 'acquired  the gathering of the crop   c   are all referred to the determination of the jury  who are instructed ', '')], [('until', '4 The statute which presents the question before the court is  that  it shall not be lawful for any sheriff or other officer to levy a writ of fieei facias or other execution  on the planted crop of a debtor  or person against whom an execution may issue ', '')], [('subject to', 'This act must be considered in connection with the other acts upon the same subject  The policy of the State  as indicated by these statutes  is undeniably that all the property of a debtor  real and personal  to which he has a legal title  shall be', ''), ('until', 'sale by execution  and it appears to me that it would be difficult to assign a reason for the exemption of this species of property from the claims of judgment creditors  and for giving to the defendant in execution the right to dispose of it  It appears to me  with all deference  that the argument that because the sheriff was prohibited from levying on a   planted crop   that therefore the execution had lost its lien  and the debtor had the right to sell it  is a non sequitur  The mischief which the statute designed to remedy was  the sacrifice which would be necessarily made by the sale of an immature crop  the statute enables the debtor to retain it', '')], [('if', '5', ''), ('until', 'further confirmation of the correctness of this view were necessary  it will be found  I think  in the language employed by the legislature  The sheriff is forbidden to levy on a  planted crop ', ''), ('if', 'the crop is gathered  Now ', ''), ('until', 'the view taken by the majority of the court  is correct  the right secured to the plaintiff in execution  of levying on the crop after it is gathered  may be frustrated  as it was in this case  by a sale by the defendant in execution  whilst the crop is in an immature state  The construction which has been put upon the statute  involves the singular anomaly  that the legislature  for the protection of the debtor  has forbidden the plaintiff in execution to sell the property of his debtor  because it is not in a condition to bring its value  and yet permits the debtor  voluntarily  by a sale  to submit to the same sacrifice  for his own benefit  It is  in effect  a gift to the defendant in execution  of the growing crop  provided he does not gather it himself  but disposes of it in its then condition  This  I feel a thorough conviction  was not the intention of the legislature  but that it was to secure him from loss  by prohibiting a levy and sale of the crop ', ''), ('when', 'it was gathered ', '')], [('subject to', '9   Growing crops as', '')], [('subject to', 'Generally  at common law  growing crops raised by annual planting  while still attached to the soil  are regarded as personal chattels ', '')], [('where', 'Parol evidence is not admissible to contradict  or substantially vary  a written contract  And', '')]]\n",
            "\n",
            "Constraints :\n",
            "\n",
            "[[('after', 'under st 1821  prohibiting a levy on a crop until it has been gathered  no lien attaches in favor of a fi  fa  on a growing crop  nor does such lien attach until', '')], [('first of', '1 this was a trial of the right of property under the statute  in november  1840  an execution issued from the circuit court of sumter  at the suit of the plaintiff in error  requiring the sheriff of that county  to make of the goods   c   of allen harrison and others  the sum of thirty seven hundred and seventy seven 80 100 dollars  besides costs  this execution was levied on thirty bales of cotton  as the property of allen harrison  which was claimed  and a bond given to try the right  an issue was made up to try the question of the liability of the cotton to the plaintiff s execution  and submitted to a jury  on the trial  a bill of exceptions was sealed at the instance of the plaintiff  the plaintiff proved that he recovered his judgment in october  1839  that an execution  741 was issued thereon on the 7th nov  thereafter  and that alias and pluries fieri facias   issued regularly up to the time levy was made  that the cotton levied on was growed on the plantation of harrison  and cultivated by the hands in his service  it was proved by the claimants  by the production of a written contract  that harrison  on the twenty second of may  1840  in consideration that the claimants were involved  as indorsers for burton   harrison of sumter county  and were then exposed to an execution  amounting to upwards of fourteen thousand dollars  bargained and sold to the claimants all his growing crop of cotton  c   consisting of one hundred and twenty acres   c  allen harrison promised and obliged himself to give up his crop to the use of the claimants at any time to save them from suffering as his indorsers  if the crop matured and was gathered he undertook to deliver the cotton in gainesville  the claimants came from tennessee   where they resided  about the', '')], [('first of', 'the court charged the jury  that the plaintiff had no lien by virtue of his judgment  and execution on the growing crop  that harrison had a right to convey it  without being in any manner restrained by them  that the writing adduced  was a sale of the crop  but if it was not  and the lien of the fieri facias would have attached upon it  when gathered  yet if the claimants obtained possession on the', '')], [('after', 'r  h  smith  for the plaintiff  in error  made the following points   1  the crop of harrison  must  in may  1840  have been in an immature state  and it is insisted  was not the subject of a sale  2  by the common law  a growing crop could be levied on and sold   1 salk  rep  361  1 bos    p  rep  307  6 east s rep  604  note 1  2 johns  rep  418  422  7 mass  rep  34   and our statue     aik  dig    41  p  167   which forbids the levy of an  742 execution on a growing crop  is to receive a strict construction  it merely inhibits the levy  but the lien attaches  and a levy and sale may be made', '')], [('more than', 'the claimants had previous to the levy of the execution taken possession of the crop  prepared the cotton for market  and removed it to a ware house  this possession  it is insisted  was a trespass  because it was acquired in the absence of the defendant in execution  and without his consent then given  conceding the truth of the facts stated in the bill of exceptions  and we think it will not follow  that the possession of the claimants is a nullity  and that the case must be considered as if they had never interfered with the crop  the contract contains an express undertaking to give up the crop at any time the claimants might require it for their indemnity  and if they took possession of it in the absence of the grantor   though without his consent   if he subsequently acquiesced in it  the inference would be  if necessary  that their acts were approved by him  taking this to be clear  744 law  and it will be seen  that the defendant in execution at the time of the levy had nothing', '')], [('before', '3 this brings us back to the question  whether the execution of the plaintiff was a lien on the growing crop  so as to defeat the mortgage to the claimants  it has been frequently mooted whether  at common law  corn   c  ', '')], [('before', '4 the statute which presents the question', '')], [('after', '5 if further confirmation of the correctness of this view were necessary  it will be found  i think  in the language employed by the legislature  the sheriff is forbidden to levy on a  planted crop  until the crop is gathered  now  if the view taken by the majority of the court  is correct  the right secured to the plaintiff in execution  of levying on the crop', '')], [('before', 'trover for conversion of cotton  with counts in case  appeal from the circuit court of hale  tried', '')], [('before', 'trover for conversion of cotton   appeal from the city court of montgomery  tried', '')], [('before', 'garnishment  wages  waiver of exemption   appeal from city court of montgomery  tried', '')], [('before', 'trial of right of property in cotton   appeal from the circuit court of barbour  tried', '')], [('before', 'error to the circuit court of autauga  tried', '')], [('before', 'trover for conversion of three bales of cotton  appeal from the circuit court of etowah  tried', '')]]\n",
            "\n",
            "Copyrights :\n",
            "\n",
            "None\n",
            "\n",
            "Courts :\n",
            "\n",
            "[{'enity': (1, 'United States Supreme Court', 0, [('Supreme Court', None, False, 0), (' SCOTUS', None, False, 1)]), 'alias': ('Supreme Court', None, False, 0)}]\n",
            "\n",
            "CUSIP :\n",
            "\n",
            "None\n",
            "\n",
            "Dates :\n",
            "\n",
            "[[datetime.date(1840, 11, 1), datetime.date(1839, 10, 1), datetime.date(1840, 5, 1), datetime.date(1840, 9, 1)], [datetime.date(1840, 5, 1)], [datetime.date(1840, 5, 1)], [datetime.date(2020, 12, 1)], [datetime.date(1887, 5, 1)]]\n",
            "\n",
            "Definitions :\n",
            "\n",
            "None\n",
            "\n",
            "Distances :\n",
            "\n",
            "None\n",
            "\n",
            "Durations :\n",
            "\n",
            "[[('second', 20.0, 0.00023148148148148146)], [('year', 6.0, 2190.0)]]\n",
            "\n",
            "Geographics And Geopolitics:\n",
            "\n",
            "None\n",
            "\n",
            "Money And currency :\n",
            "\n",
            "[[(100.0, 'USD'), (14000, 'USD')], [(14000, 'USD')]]\n",
            "\n",
            "Percent And Rates :\n",
            "\n",
            "None\n",
            "\n",
            "PII :\n",
            "\n",
            "None\n",
            "\n",
            "Ratios :\n",
            "\n",
            "None\n",
            "\n",
            "Regulstions :\n",
            "\n",
            "None\n",
            "\n",
            "Trademarks :\n",
            "\n",
            "None\n",
            "\n",
            "URLs :\n",
            "\n",
            "None\n",
            "\n",
            "Names: \n",
            " [['Error Circuit Court'], ['Chattel'], ['Lien'], ['Allen Harrison', 'Allen Harrison', 'Harrison', 'Harrison', 'Burton Harrison Sumter', 'Allen Harrison', 'Tennessee'], ['Harrison'], ['Harrison', 'Johns', 'Aik Dig', 'Harrison', 'Chit Con', 'Johns', 'Johns'], ['Whipple', 'Johns'], ['Chitty Con', 'Johns', 'Stewart', 'Johns', 'Lee Alston'], ['Perkins Elliott'], ['Bacon Abridgment', 'Whipple', 'Poole', 'Whipple', 'Foot', 'Aik Dig', 'Mansony Hurtell', 'Wood'], ['Clay Dig'], ['State'], ['Jones Adm'], ['Trover'], ['Lehman Durr Co'], ['Hon JOHN D'], ['Jan'], ['Janney'], ['Hon JOHN D'], ['Jan'], ['Lampley'], ['Jan'], ['Circuit Court Autauga Tried Hon'], ['Jun'], ['Bowman'], ['Court Jacob', 'Cohen', 'Court', 'Cohen'], ['Jul'], ['Trover', 'Bales Cotton'], ['Nov'], ['Weakley'], ['Referenced'], ['Page Number'], ['Sawyer'], ['Circuit Court Tuskaloosa'], ['Stewart', 'Doughty']]\n",
            "\n",
            "Address: \n",
            " None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}