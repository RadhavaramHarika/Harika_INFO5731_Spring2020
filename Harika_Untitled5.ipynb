{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harika_Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOL2Yc8XYlBBr9C3Zj3FjH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadhavaramHarika/Harika_INFO5731_Spring2020/blob/master/Harika_Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S17KQK4m4waK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "857495a6-a414-4204-e499-0182807a0a47"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as panda\n",
        "\n",
        "def getDataFromUrl(url):\n",
        "  with urllib.request.urlopen(url) as file:\n",
        "    web_content = file.read()\n",
        "    b_soup = BeautifulSoup(web_content)\n",
        "  return b_soup\n",
        "\n",
        "def getUserReviewList(urlData):\n",
        "  review_data = urlData.find_all(\"div\",{\"class\":\"lister-item-content\"})\n",
        "  reviews_list = []\n",
        "  for each in review_data:\n",
        "    title = each.find('a').get_text()\n",
        "    review = each.find(class_ = \"content\").find(class_ = \"text show-more__control\").get_text()\n",
        "    reviews_list.append([title, review])\n",
        "  return reviews_list\n",
        "\n",
        "def getHundredUserReviewList(urlData):\n",
        "  total_reviews = getUserReviewList(urlData)\n",
        "  count=len(total_reviews)\n",
        "  base_url = loadMoreReviews(urlData)\n",
        "  while len(total_reviews)!=100:\n",
        "    if base_url is not None:\n",
        "      content = getDataFromUrl(base_url)\n",
        "      for each in getUserReviewList(content):\n",
        "        total_reviews.append(each)\n",
        "    else:\n",
        "      break\n",
        "    count = len(total_reviews)\n",
        "  return total_reviews\n",
        "\n",
        "def extractFirstReview(reviewList):\n",
        "  user_review = reviewList[0]\n",
        "  return user_review\n",
        "\n",
        "def retriewTitles(reviewList):\n",
        "  titles = [each.find('a').get_text() for each in reviewList]\n",
        "  return titles \n",
        "\n",
        "def retriewTotalReview(reviewList):\n",
        "  totalReviews = [each.find(class_ = \"content\").find(class_ = \"text show-more__control\").get_text() for each in reviewList]\n",
        "  return totalReviews \n",
        "\n",
        "def loadMoreReviews(url_data):\n",
        "  moreReviews = url_data.select(\".load-more-data\") if url_data else None\n",
        "  if len(moreReviews):\n",
        "    ajax_url = moreReviews[0][\"data-ajaxurl\"]\n",
        "    url = \"https://www.imdb.com\"+ajax_url+\"?ref_=undefined&paginationKey=\"\n",
        "    try:    \n",
        "      key = moreReviews[0][\"data-key\"]\n",
        "    except KeyError:\n",
        "      key = None\n",
        "    print(\"K\",key)\n",
        "  return url+key if key else None\n",
        "\n",
        "def uploadReviewsToCSV(reviewList):\n",
        "  csv_dict = {\"Review Title\":[each[0] for each in reviewList],\n",
        "              \"Total Review\":[each[1] for each in reviewList]}\n",
        "  datafr = panda.DataFrame(csv_dict)\n",
        "  datafr.to_csv('UserReview.csv', index = False)\n",
        "  return datafr\n",
        "\n",
        "\n",
        "url = \"https://www.imdb.com/title/tt7286456/reviews?start=0\"\n",
        "web_content = getDataFromUrl(url)\n",
        "\n",
        "user_reviews = getHundredUserReviewList(web_content)\n",
        "print(len(user_reviews))\n",
        "\n",
        "first_review = extractFirstReview(user_reviews)\n",
        "print(first_review)\n",
        "\n",
        "uploadReviewsToCSV(user_reviews)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K g4wp7crnq4ytiyid7kxxrojwrprmsazhzfmxvlnomwklyczuf43o6ss6oiyvvmzndj4k4u5ja6rjnzuwxlyxe75bw2mfnzy\n",
            "100\n",
            "[\" As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\\n\", \"I was a person that saw all the hype and claims of masterpiece as overreacting and overblown excitement for another Joker based film. I thought this looked solid at best and even a bit too pretentious in the trailer, but in here to say I was incredibly wrong. This is a massive achievement of cinema that's extremely rare in a day and age of cgi nonsense and reboots. While this is somewhat of a reboot of sorts, the standalone origin tale is impeccable from start to finish and echoes resemblance to the best joker origin comics from the past. Joaquin bleeds, sweats, and cries his every drop into this magnificently dedicated performance. Heath Ledger would be proud. This is undoubtedly the greatest acting performance since Heath's joker. The directing and writing is slickly brilliant and the bleak settings and tones are palpable throughout. When this film was over the place was blown away and every audience member was awestruck that they witnessed a film that could still transport them into a character's world and very existence. Believe the hype. This is going to be revered as a transcending masterpiece of cinema.\"]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Title</th>\n",
              "      <th>Total Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wi...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate\\n</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perfect in every aspect.\\n</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real\\n</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Astonishing Masterpiece\\n</td>\n",
              "      <td>What an incredible ride this was. I was almost...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Nonsense plot\\n</td>\n",
              "      <td>Arthur Fleck lives with his invalid mother. He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>I feel like everyone is brain washed\\n</td>\n",
              "      <td>I thought this movie was complete and utter ga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I made an account just to rate my disappoint ...</td>\n",
              "      <td>I was expecting a masterpiece and oscar worthy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Stop comparing it with Endgame\\n</td>\n",
              "      <td>Joker is literally an achievement in cinematic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Title                                       Total Review\n",
              "0    As a viewer that actually went to TIFF and wi...  I was a person that saw all the hype and claim...\n",
              "1    Outstanding movie with a haunting performance...  Every once in a while a movie comes, that trul...\n",
              "2                    Only certain people can relate\\n  This is a movie that only those who have felt ...\n",
              "3                          Perfect in every aspect.\\n  Truly a masterpiece, The Best Hollywood film o...\n",
              "4                                  The Hype is real\\n  Most of the time movies are anticipated like t...\n",
              "..                                                ...                                                ...\n",
              "95                          Astonishing Masterpiece\\n  What an incredible ride this was. I was almost...\n",
              "96                                    Nonsense plot\\n  Arthur Fleck lives with his invalid mother. He...\n",
              "97             I feel like everyone is brain washed\\n  I thought this movie was complete and utter ga...\n",
              "98   I made an account just to rate my disappoint ...  I was expecting a masterpiece and oscar worthy...\n",
              "99                   Stop comparing it with Endgame\\n  Joker is literally an achievement in cinematic...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W70ezX0WTAyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "c4567e82-1a1a-4264-c55a-0da9869e08e1"
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "import csv,os,math,unicodedata,re\n",
        "from google.colab import drive,files\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter \n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as panda\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string,io\n",
        "from google.colab import drive,files\n",
        "\n",
        "def remove_punctuations(samp_sent):\n",
        "  removed_punct = []\n",
        "  translator = str.maketrans('', '', string.punctuation) \n",
        "  for each in samp_sent:\n",
        "    removed_punct.append(each.translate(translator))\n",
        "  return removed_punct\n",
        "\n",
        "def remove_numbers(samp_sent):\n",
        "  removed_numb = []\n",
        "  for each in samp_sent:\n",
        "    removed_numb.append(re.sub(r\"\\d\", \"\", each))\n",
        "  return removed_numb\n",
        "\n",
        "def remove_stop_words(samp_sent):\n",
        "  stops = stopwords.words('english')\n",
        "  no_stops = []\n",
        "  for each in samp_sent:\n",
        "    words = each.split()\n",
        "    no_stops.append(' '.join([w for w in words\n",
        "                              if w not in stops]))\n",
        "  return no_stops\n",
        "\n",
        "def lower_sentences(samp_sent):\n",
        "  lowers = []\n",
        "  for each in samp_sent:\n",
        "    lowers.append(each.lower())\n",
        "  return lowers\n",
        "\n",
        "def stemming(samp_sent_tokens):\n",
        "  stem_sent = []\n",
        "  ps = PorterStemmer()\n",
        "  for each in samp_sent_tokens:\n",
        "    words = word_tokenize(each)\n",
        "    stem_sent.append([ps.stem(w) for w in words])\n",
        "  return stem_sent\n",
        "\n",
        "def lemmatized_sent_tokens(samp_sent_tokens):\n",
        "  lmtzr = WordNetLemmatizer() \n",
        "  lem_sent_tokens = []\n",
        "  for each in samp_sent_tokens:\n",
        "    lem_sent_tokens.append([lmtzr.lemmatize(w) for w in each])\n",
        "  return lem_sent_tokens\n",
        "\n",
        "def uploadPreprocessedDataToCSV(reviews, data):\n",
        "  no_punct_sent = remove_punctuations(reviews)\n",
        "  no_stop_sent = remove_stop_words(no_punct_sent)\n",
        "  no_numb_sent = remove_numbers(no_stop_sent)\n",
        "  lowered_sent = lower_sentences(no_numb_sent)\n",
        "  stem_sent_tokens = stemming(lowered_sent)\n",
        "  lemmatized_tokens = lemmatized_sent_tokens(stem_sent_tokens)\n",
        "  #[word_tokenize(each) for each in lowered_sent]\n",
        "  data[\"Preprocessed Reviews\"] = lemmatized_tokens\n",
        "  data.to_csv(\"UserReview.csv\")\n",
        "  uploading_directory(data)\n",
        "  return data\n",
        "\n",
        "def uploading_directory(data):\n",
        "  drive.mount('/content/gdrive')\n",
        "  filenames = os.listdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "  print(filenames)\n",
        "\n",
        "  os.chdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "  text_dict = ['Review Title','Total Review','Preprocessed Reviews']\n",
        "  for i in range(len(data)):\n",
        "    text_dict.append([data['Review Title'][i],data['Total Review'][i],data['Preprocessed Reviews'][i]])\n",
        "\n",
        "  with open('UserReviews.csv', 'w') as csvFile:\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "    csvWriter.writerows(text_dict)\n",
        "    csvFile.close()\n",
        "  return\n",
        "  \n",
        "data = panda.read_csv('UserReview.csv')\n",
        "uploadPreprocessedDataToCSV(data['Total Review'],data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['Untitled0.ipynb', 'My_first_in_Class_exercise.ipynb', 'My_First_in_class_Assignement_Harika.ipynb', 'Untitled1.ipynb', 'exercise_02_data_collection', 'Copy of In_class_exercise_02 (2).ipynb', 'Untitled2.ipynb', 'textContents.csv', 'Copy of In_class_exercise_02 (1).ipynb', 'Harika_In_class_exercise_02.ipynb', 'Assignment1_denton_housing.csv', 'Harika_INFO5731_Assignment_One.ipynb', 'Untitled', 'Untitled3.ipynb', 'Harika_In_class_exercise_03 (1).ipynb', 'Harika_In_class_exercise_03.ipynb', 'Legal_case.txt', 'stopwords', 'Lower.csv', 'Clean.csv', 'Untitled4.ipynb', 'Copy of In_class_exercise_02.ipynb', 'UserReviews.xlsx', 'Harika_In-class-exercise-04.ipynb', 'Harika_In_class_exercise_05.ipynb', 'Harika_INFO5731_Assignment_Two.ipynb', 'Copy of In_class_exercise_01.ipynb', 'Harika_INFO5731_Assignment_Three.ipynb', 'Harika_Untitled5.ipynb', 'UserReview.csv']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Title</th>\n",
              "      <th>Total Review</th>\n",
              "      <th>Preprocessed Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wi...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>[i, person, saw, hype, claim, masterpiec, over...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>[everi, movi, come, truli, make, impact, joaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate\\n</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>[thi, movi, felt, alon, isol, truli, relat, yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perfect in every aspect.\\n</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>[truli, masterpiec, the, best, hollywood, film...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real\\n</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>[most, time, movi, anticip, like, end, fall, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Astonishing Masterpiece\\n</td>\n",
              "      <td>What an incredible ride this was. I was almost...</td>\n",
              "      <td>[what, incred, ride, i, almost, motionless, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Nonsense plot\\n</td>\n",
              "      <td>Arthur Fleck lives with his invalid mother. He...</td>\n",
              "      <td>[arthur, fleck, live, invalid, mother, he, suf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>I feel like everyone is brain washed\\n</td>\n",
              "      <td>I thought this movie was complete and utter ga...</td>\n",
              "      <td>[i, thought, movi, complet, utter, garbag, i, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I made an account just to rate my disappoint ...</td>\n",
              "      <td>I was expecting a masterpiece and oscar worthy...</td>\n",
              "      <td>[i, expect, masterpiec, oscar, worthi, film, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Stop comparing it with Endgame\\n</td>\n",
              "      <td>Joker is literally an achievement in cinematic...</td>\n",
              "      <td>[joker, liter, achiev, cinemat, histori, there...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Title  ...                               Preprocessed Reviews\n",
              "0    As a viewer that actually went to TIFF and wi...  ...  [i, person, saw, hype, claim, masterpiec, over...\n",
              "1    Outstanding movie with a haunting performance...  ...  [everi, movi, come, truli, make, impact, joaqu...\n",
              "2                    Only certain people can relate\\n  ...  [thi, movi, felt, alon, isol, truli, relat, yo...\n",
              "3                          Perfect in every aspect.\\n  ...  [truli, masterpiec, the, best, hollywood, film...\n",
              "4                                  The Hype is real\\n  ...  [most, time, movi, anticip, like, end, fall, s...\n",
              "..                                                ...  ...                                                ...\n",
              "95                          Astonishing Masterpiece\\n  ...  [what, incred, ride, i, almost, motionless, th...\n",
              "96                                    Nonsense plot\\n  ...  [arthur, fleck, live, invalid, mother, he, suf...\n",
              "97             I feel like everyone is brain washed\\n  ...  [i, thought, movi, complet, utter, garbag, i, ...\n",
              "98   I made an account just to rate my disappoint ...  ...  [i, expect, masterpiec, oscar, worthi, film, h...\n",
              "99                   Stop comparing it with Endgame\\n  ...  [joker, liter, achiev, cinemat, histori, there...\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdvRjnLjKcEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "abade7f8-6dfd-43f0-b5f5-777eecfa6757"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from spacy.gold import GoldParse \n",
        "from spacy.language import EntityRecognizer\n",
        "\n",
        "\n",
        "def posTagging(samp_sent):\n",
        "  pos_tags = []\n",
        "  print(\"\\nQues 3.1\")\n",
        "  print(\"Nouns\\tVerbs\\tAdjectives\\tAdverbs\")\n",
        "  for each in samp_sent:\n",
        "    tags = nltk.pos_tag(' '.join(each))\n",
        "    pos_tags.append(tags)\n",
        "    num_N = ' '.join([w[1] for w in tags]).count('NN')\n",
        "    num_V = ' '.join([w[1] for w in tags]).count('VBP')\n",
        "    num_Adj = ' '.join([w[1] for w in tags]).count('JJ')\n",
        "    num_Adv = ' '.join([w[1] for w in tags]).count('RB')\n",
        "    print(str(num_N) +'\\t'+str(num_V)+'\\t'+str(num_Adj)+'\\t\\t'+str(num_Adv))\n",
        "  return\n",
        "\n",
        "def constituencyParsing(samp_Sent):\n",
        "  pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "  cp = nltk.chunk.regexp.RegexpParser(pattern)\n",
        "  print(\"\\nQues 3.2\")\n",
        "  print(\"Constituency Parsing Tree for all the sentences\")\n",
        "  for each in samp_Sent:\n",
        "    cs = cp.parse(nltk.pos_tag(each))\n",
        "    #print(cs)\n",
        "    cs.pprint()\n",
        "  return\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "  if node.n_lefts + node.n_rights > 0:\n",
        "      return Tree('-'.join([node.orth_,node.tag_]), [to_nltk_tree(child) for child in node.children])\n",
        "  else:\n",
        "      return '-'.join([node.orth_,node.tag_])\n",
        "\n",
        "def dependencyParsingTree(samp_sent):\n",
        "  print(\"\\nQues 3.2\")\n",
        "  print(\"\\nDependency Parsing Tree for all the sentences\")\n",
        "  nlp = spacy.load('en')\n",
        "  for each in samp_sent:\n",
        "    if len(each)>1:\n",
        "      e_sent = nlp(' '.join(each))\n",
        "      [to_nltk_tree(sent.root).pretty_print() for sent in e_sent.sents]\n",
        "  return\n",
        "\n",
        "def nameEntityRecog(samp_sent):\n",
        "  print(\"\\nQues 3.3\")\n",
        "  print(\"\\nEntity Recognition\")\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  for each in samp_sent:\n",
        "    doc = nlp(' '.join(each))\n",
        "    for entity in doc.ents:\n",
        "      print(entity.text, entity.label_) \n",
        "  return\n",
        " \n",
        "  '''nlp = spacy.load('en', entity = False, parser = False) \n",
        "  \n",
        "  doc_list = [] \n",
        "  doc = nlp('Llamas make great pets.') \n",
        "  doc_list.append(doc) \n",
        "  gold_list = [] \n",
        "  gold_list.append(GoldParse(doc, [u'ANIMAL', u'O', u'O', u'O'])) \n",
        "    \n",
        "  ner = EntityRecognizer(nlp.vocab, entity_types = ['ANIMAL']) \n",
        "  ner.update(doc_list, gold_list)'''\n",
        "\n",
        "posTagging(data[\"Preprocessed Reviews\"])\n",
        "constituencyParsing(data[\"Preprocessed Reviews\"])\n",
        "dependencyParsingTree(data[\"Preprocessed Reviews\"])\n",
        "nameEntityRecog(data[\"Preprocessed Reviews\"])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-579e7fa0a126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconlltags2tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'EntityRecognizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}