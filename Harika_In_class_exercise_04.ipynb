{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harika_In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadhavaramHarika/Harika_INFO5731_Spring2020/blob/master/Harika_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw",
        "colab_type": "text"
      },
      "source": [
        "# **The fourth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f",
        "colab_type": "text"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 2-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab_type": "code",
        "outputId": "8f9dc444-a204-43d0-d2e8-a65bedab32db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Write your code here\n",
        "import csv,os,math,unicodedata,re\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive,files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter \n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "file_list = os.listdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks')\n",
        "\n",
        "def open_textfile():\n",
        "  if 'Legal_case.txt' in file_list:\n",
        "    text_data = open('Legal_case.txt').read()\n",
        "  clean_data = text_data.replace('\\xa0','')\n",
        "  return clean_data\n",
        "\n",
        "def basic_featExtraction(file_data):\n",
        "  data_dict = {}\n",
        "  text_words = clean_data.split()\n",
        "  text_sent = [each for each in clean_data.split('\\n')\n",
        "                  if each]\n",
        "  print(\"Number of sentences: \",len(text_sent))\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "\n",
        "  for each_sent in text_sent:\n",
        "    count = stop_count = spl_c = num_c = up_c = 0\n",
        "    words = each_sent.split()\n",
        "    for each in word_tokenize(each_sent):\n",
        "      if each.isdigit():\n",
        "        num_c += 1\n",
        "    for each_word in words:\n",
        "      spl_w = re.sub(r\"\\w\",\"\", each_word, flags = re.I)\n",
        "      if spl_w:\n",
        "        spl_c += len(spl_w)\n",
        "      count += len(each_word)\n",
        "      if each_word.lower() in stop_words:\n",
        "        stop_count += 1\n",
        "      if each_word.isupper():\n",
        "        up_c += 1\n",
        "    data_dict[each_sent] = [len(words),count,count/len(words), stop_count, spl_c, num_c,up_c]\n",
        "    return data_dict\n",
        "\n",
        "def uploadToCSV(samp_dict):\n",
        "  datafrm = panda.DataFrame({\"Sentences\": data_dict.keys(),\n",
        "                            \"Number of words\": [data_dict[k][0] for k,v in data_dict.items()],\n",
        "                            \"Number of Characters without spaces\":[data_dict[k][1] for k,v in data_dict.items()],\n",
        "                            \"Avg Word Length\":[data_dict[k][2] for k,v in data_dict.items()],\n",
        "                            \"Number of stopwords\":[data_dict[k][3] for k,v in data_dict.items()],\n",
        "                            \"Number of special chars\":[data_dict[k][4] for k,v in data_dict.items()],\n",
        "                            \"Number of numerics\":[data_dict[k][5] for k,v in data_dict.items()],\n",
        "                            \"Number of uppercase words\":[data_dict[k][6] for k,v in data_dict.items()]})\n",
        "\n",
        "  datafrm.to_csv(\"Clean.csv\")\n",
        "  return datafrm\n",
        "\n",
        "cleaned_text = open_textfile()\n",
        "csv_dict = basic_featExtraction(cleaned_text)\n",
        "uploadToCSV(csv_dict)\n",
        "\n",
        "#1.\n",
        "\n",
        "def freq_words():\n",
        "  split_it = cleaned_text.split() \n",
        "  counter = Counter(split_it)\n",
        "  most_occur = {}\n",
        "  for each in counter.most_common(10):\n",
        "    most_occur[each[0]] = each[1]\n",
        "  return most_occur\n",
        "\n",
        "def rare_words():\n",
        "  split_it = cleaned_text.split() \n",
        "  counter = Counter(split_it)\n",
        "  rare_occur = {}\n",
        "  for each in counter.rare_words(10):\n",
        "    rare_occur[each[0]] = each[1]\n",
        "  return rare_occur\n",
        "\n",
        "freq_words()\n",
        "\n",
        "def basicTextPreProcessing(clean_data):\n",
        "  stop_words = stopwords.words('english')\n",
        "  ps = PorterStemmer() \n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  text_sent = [each for each in clean_data.split('\\n')\n",
        "                  if each]\n",
        "  lower_cased = punct_rem = no_stop_words = no_freq =  rare_occur = correct_spel =stem_words =lem_words = tokens = []\n",
        "    for each_s in text_sent:\n",
        "    punct_rem.append(re.sub(r\"\\W\",\" \", each_s, flags =re.I))\n",
        "    words = each_s.split()\n",
        "    temp = []\n",
        "    t_l = []\n",
        "    no_f = rare_o = c_s = s_w = l_w =[]\n",
        "    for each_w in words:\n",
        "      temp.append(each_w.lower())\n",
        "      tokens.append(word_tokenize(each_s))\n",
        "      s_w.append(ps.stem(each_w))\n",
        "      l_w.append(lemmatizer.lemmatize(each_w))\n",
        "      if each_w not in stop_words:\n",
        "        t_l.append(each_w)\n",
        "      if each_w not in freq_words.keys():\n",
        "        no_f.append(each_w)\n",
        "      if each_w not in rare_words().keys():\n",
        "        rare_o.append(each_w)\n",
        "      if str(TextBlob(each_w).correct():\n",
        "        c_s.append(each_w)\n",
        "      \n",
        "    stem_words.append(\" \".join(s_w))\n",
        "    correct_spel.append(\" \".join(c_s))\n",
        "    lem_words.append(\" \".join(l_w))\n",
        "    no_freq.append(\" \".join(no_f))\n",
        "    no_stop_words.append(\" \".join(t_l)) \n",
        "    rare_occur.append(\" \".join(rare_o))\n",
        "    lower_cased.append(' '.join(temp))\n",
        "  return (lower_cased,punct_rem,no_stop_words,no_freq,rare_occur, correct_spel, stem_words, tokens)\n",
        "\n",
        "def uploadToCSV(samp_dict):\n",
        "  datafrm = panda.DataFrame({\"Sentences\": data_dict.keys(),\n",
        "                            \"Lower Cased\": [data_dict[k][0] for k,v in data_dict.items()],\n",
        "                            \"Punctuations removed\":[data_dict[k][1] for k,v in data_dict.items()],\n",
        "                            \"No stop words\":[data_dict[k][2] for k,v in data_dict.items()],\n",
        "                            \"No Frequent words\":[data_dict[k][3] for k,v in data_dict.items()],\n",
        "                            \"Rare occurences\":[data_dict[k][4] for k,v in data_dict.items()],\n",
        "                            \"Number of numerics\":[data_dict[k][5] for k,v in data_dict.items()],\n",
        "                            \"Number of uppercase words\":[data_dict[k][6] for k,v in data_dict.items()]})\n",
        "\n",
        "  datafrm.to_csv(\"Clean.csv\")\n",
        "  return datafrm\n",
        "\n",
        "cleaned_text = open_textfile()\n",
        "csv_dict = basic_featExtraction(cleaned_text)\n",
        "uploadToCSV(csv_dict)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Number of sentences:  147\n",
            "{'the': 307, 'of': 138, 'to': 114, 'and': 81, 'a': 77, 'in': 77, 'it': 59, 'that': 55, 'was': 52, 'is': 48}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV",
        "colab_type": "text"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. \n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab_type": "code",
        "outputId": "fc6bce4c-1a93-4aec-a21a-150b0c82e369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "ip_no_zeros = []\n",
        "for each in ip.split('.'):\n",
        "  ip_no_zeros.append(re.sub(r\"^0+\", \"\", each))\n",
        "  \n",
        "print(\"\\nGiven input IP address :\",ip)  \n",
        "print(\"\\nFinal IP address after removing leading zeros is :\",'.'.join(ip_no_zeros))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Given input IP address : 260.08.094.109\n",
            "\n",
            "Final IP address after removing leading zeros is : 260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab_type": "code",
        "outputId": "75b2b363-959b-4f1e-d1a3-98e023e8b71d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "# Write your code here\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "def extract_years(samp_sentence):\n",
        "  num_sent = re.sub(r\"[a-zA-Z]\", \"\", samp_sentence, flags=re.I)\n",
        "  sentn_list = word_tokenize(num_sent)\n",
        "  num_list = [int(each) for each in sentn_list \n",
        "              if each.isnumeric() and int(each)>1500]\n",
        "  \n",
        "  return num_list\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have \\\n",
        "happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing \\\n",
        "stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' \\\n",
        "soccer team from a flooded cave to the divisive election of President Donald Trump.\"  \n",
        "print(\"\\nGiven sentence is : \\n\",sentence)\n",
        "print(\"\\nResultant years from above sentence are :\\n\",extract_years(sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "Given sentence is : \n",
            " The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\n",
            "\n",
            "Resultant years from above sentence are :\n",
            " [2010, 2010, 2019]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}