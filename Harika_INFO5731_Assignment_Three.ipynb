{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harika_INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadhavaramHarika/Harika_INFO5731_Spring2020/blob/master/Harika_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG4o4OlPKAKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "ff013970-b42a-4497-9ad3-f81f2b5bd0e2"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as panda\n",
        "\n",
        "def getDataFromUrl(url):\n",
        "  with urllib.request.urlopen(url) as file:\n",
        "    web_content = file.read()\n",
        "    b_soup = BeautifulSoup(web_content)\n",
        "  return b_soup\n",
        "\n",
        "def getUserReviewList(urlData):\n",
        "  review_data = urlData.find_all(\"div\",{\"class\":\"lister-item-content\"})\n",
        "  reviews_list = []\n",
        "  for each in review_data:\n",
        "    title = each.find('a').get_text()\n",
        "    review = each.find(class_ = \"content\").find(class_ = \"text show-more__control\").get_text()\n",
        "    reviews_list.append([title, review])\n",
        "  return reviews_list\n",
        "\n",
        "def getHundredUserReviewList(urlData):\n",
        "  total_reviews = getUserReviewList(urlData)\n",
        "  count=len(total_reviews)\n",
        "  base_url = loadMoreReviews(urlData)\n",
        "  while len(total_reviews)!=100:\n",
        "    if base_url is not None:\n",
        "      content = getDataFromUrl(base_url)\n",
        "      for each in getUserReviewList(content):\n",
        "        total_reviews.append(each)\n",
        "    else:\n",
        "      break\n",
        "    count = len(total_reviews)\n",
        "  return total_reviews\n",
        "\n",
        "def extractFirstReview(reviewList):\n",
        "  user_review = reviewList[0]\n",
        "  return user_review\n",
        "\n",
        "def retriewTitles(reviewList):\n",
        "  titles = [each.find('a').get_text() for each in reviewList]\n",
        "  return titles \n",
        "\n",
        "def retriewTotalReview(reviewList):\n",
        "  totalReviews = [each.find(class_ = \"content\").find(class_ = \"text show-more__control\").get_text() for each in reviewList]\n",
        "  return totalReviews \n",
        "\n",
        "def loadMoreReviews(url_data):\n",
        "  moreReviews = url_data.select(\".load-more-data\") if url_data else None\n",
        "  if len(moreReviews):\n",
        "    ajax_url = moreReviews[0][\"data-ajaxurl\"]\n",
        "    url = \"https://www.imdb.com\"+ajax_url+\"?ref_=undefined&paginationKey=\"\n",
        "    try:    \n",
        "      key = moreReviews[0][\"data-key\"]\n",
        "    except KeyError:\n",
        "      key = None\n",
        "  return url+key if key else None\n",
        "\n",
        "def uploadReviewsToCSV(reviewList):\n",
        "  csv_dict = {\"Review Title\":[each[0] for each in reviewList],\n",
        "              \"Total Review\":[each[1] for each in reviewList]}\n",
        "  datafr = panda.DataFrame(csv_dict)\n",
        "  datafr.to_csv('UserReview.csv', index = False)\n",
        "  return datafr\n",
        "\n",
        "\n",
        "url = \"https://www.imdb.com/title/tt7286456/reviews?start=0\"\n",
        "web_content = getDataFromUrl(url)\n",
        "\n",
        "user_reviews = getHundredUserReviewList(web_content)\n",
        "print(len(user_reviews))\n",
        "\n",
        "first_review = extractFirstReview(user_reviews)\n",
        "print(first_review)\n",
        "\n",
        "uploadReviewsToCSV(user_reviews)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "[\" As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\\n\", \"I was a person that saw all the hype and claims of masterpiece as overreacting and overblown excitement for another Joker based film. I thought this looked solid at best and even a bit too pretentious in the trailer, but in here to say I was incredibly wrong. This is a massive achievement of cinema that's extremely rare in a day and age of cgi nonsense and reboots. While this is somewhat of a reboot of sorts, the standalone origin tale is impeccable from start to finish and echoes resemblance to the best joker origin comics from the past. Joaquin bleeds, sweats, and cries his every drop into this magnificently dedicated performance. Heath Ledger would be proud. This is undoubtedly the greatest acting performance since Heath's joker. The directing and writing is slickly brilliant and the bleak settings and tones are palpable throughout. When this film was over the place was blown away and every audience member was awestruck that they witnessed a film that could still transport them into a character's world and very existence. Believe the hype. This is going to be revered as a transcending masterpiece of cinema.\"]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Title</th>\n",
              "      <th>Total Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wi...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate\\n</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perfect in every aspect.\\n</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real\\n</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Astonishing Masterpiece\\n</td>\n",
              "      <td>What an incredible ride this was. I was almost...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Nonsense plot\\n</td>\n",
              "      <td>Arthur Fleck lives with his invalid mother. He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>I feel like everyone is brain washed\\n</td>\n",
              "      <td>I thought this movie was complete and utter ga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I made an account just to rate my disappoint ...</td>\n",
              "      <td>I was expecting a masterpiece and oscar worthy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Stop comparing it with Endgame\\n</td>\n",
              "      <td>Joker is literally an achievement in cinematic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Title                                       Total Review\n",
              "0    As a viewer that actually went to TIFF and wi...  I was a person that saw all the hype and claim...\n",
              "1    Outstanding movie with a haunting performance...  Every once in a while a movie comes, that trul...\n",
              "2                    Only certain people can relate\\n  This is a movie that only those who have felt ...\n",
              "3                          Perfect in every aspect.\\n  Truly a masterpiece, The Best Hollywood film o...\n",
              "4                                  The Hype is real\\n  Most of the time movies are anticipated like t...\n",
              "..                                                ...                                                ...\n",
              "95                          Astonishing Masterpiece\\n  What an incredible ride this was. I was almost...\n",
              "96                                    Nonsense plot\\n  Arthur Fleck lives with his invalid mother. He...\n",
              "97             I feel like everyone is brain washed\\n  I thought this movie was complete and utter ga...\n",
              "98   I made an account just to rate my disappoint ...  I was expecting a masterpiece and oscar worthy...\n",
              "99                   Stop comparing it with Endgame\\n  Joker is literally an achievement in cinematic...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9KUHRwSKF3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "206e1a10-d43b-4645-93b5-d46042b7f378"
      },
      "source": [
        "import nltk\n",
        "import csv,os,math,unicodedata,re\n",
        "from google.colab import drive,files\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter \n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as panda\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string,io\n",
        "from google.colab import drive,files\n",
        "\n",
        "def remove_punctuations(samp_sent):\n",
        "  removed_punct = []\n",
        "  translator = str.maketrans('', '', string.punctuation) \n",
        "  for each in samp_sent:\n",
        "    removed_punct.append(each.translate(translator))\n",
        "  return removed_punct\n",
        "\n",
        "def remove_numbers(samp_sent):\n",
        "  removed_numb = []\n",
        "  for each in samp_sent:\n",
        "    removed_numb.append(re.sub(r\"\\d\", \"\", each))\n",
        "  return removed_numb\n",
        "\n",
        "def remove_stop_words(samp_sent):\n",
        "  stops = stopwords.words('english')\n",
        "  no_stops = []\n",
        "  for each in samp_sent:\n",
        "    words = each.split()\n",
        "    no_stops.append(' '.join([w for w in words\n",
        "                              if w not in stops]))\n",
        "  return no_stops\n",
        "\n",
        "def lower_sentences(samp_sent):\n",
        "  lowers = []\n",
        "  for each in samp_sent:\n",
        "    lowers.append(each.lower())\n",
        "  return lowers\n",
        "\n",
        "def stemming(samp_sent_tokens):\n",
        "  stem_sent = []\n",
        "  ps = PorterStemmer()\n",
        "  for each in samp_sent_tokens:\n",
        "    words = word_tokenize(each)\n",
        "    stem_sent.append([ps.stem(w) for w in words])\n",
        "  return stem_sent\n",
        "\n",
        "def lemmatized_sent_tokens(samp_sent_tokens):\n",
        "  lmtzr = WordNetLemmatizer() \n",
        "  lem_sent_tokens = []\n",
        "  for each in samp_sent_tokens:\n",
        "    lem_sent_tokens.append([lmtzr.lemmatize(w) for w in each])\n",
        "  return lem_sent_tokens\n",
        "\n",
        "def uploadPreprocessedDataToCSV(reviews, data):\n",
        "  no_punct_sent = remove_punctuations(reviews)\n",
        "  no_stop_sent = remove_stop_words(no_punct_sent)\n",
        "  no_numb_sent = remove_numbers(no_stop_sent)\n",
        "  lowered_sent = lower_sentences(no_numb_sent)\n",
        "  stem_sent_tokens = stemming(lowered_sent)\n",
        "  lemmatized_tokens = lemmatized_sent_tokens(stem_sent_tokens)\n",
        "  #[word_tokenize(each) for each in lowered_sent]\n",
        "  data[\"Preprocessed Reviews\"] = [' '.join(each) for each in lemmatized_tokens]\n",
        "  data.to_csv('UserReview.csv',index = False)\n",
        "  \n",
        "  return data\n",
        "  \n",
        "data = panda.read_csv('UserReview.csv')\n",
        "uploadPreprocessedDataToCSV(data['Total Review'],data)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review Title</th>\n",
              "      <th>Total Review</th>\n",
              "      <th>Preprocessed Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wi...</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>i person saw hype claim masterpiec overreact o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance...</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>everi movi come truli make impact joaquin perf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate\\n</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>thi movi felt alon isol truli relat you unders...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Perfect in every aspect.\\n</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>truli masterpiec the best hollywood film one b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real\\n</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>most time movi anticip like end fall short way...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Astonishing Masterpiece\\n</td>\n",
              "      <td>What an incredible ride this was. I was almost...</td>\n",
              "      <td>what incred ride i almost motionless throughou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Nonsense plot\\n</td>\n",
              "      <td>Arthur Fleck lives with his invalid mother. He...</td>\n",
              "      <td>arthur fleck live invalid mother he suffer men...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>I feel like everyone is brain washed\\n</td>\n",
              "      <td>I thought this movie was complete and utter ga...</td>\n",
              "      <td>i thought movi complet utter garbag i dont und...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>I made an account just to rate my disappoint ...</td>\n",
              "      <td>I was expecting a masterpiece and oscar worthy...</td>\n",
              "      <td>i expect masterpiec oscar worthi film hype i a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Stop comparing it with Endgame\\n</td>\n",
              "      <td>Joker is literally an achievement in cinematic...</td>\n",
              "      <td>joker liter achiev cinemat histori there doubt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Review Title  ...                               Preprocessed Reviews\n",
              "0    As a viewer that actually went to TIFF and wi...  ...  i person saw hype claim masterpiec overreact o...\n",
              "1    Outstanding movie with a haunting performance...  ...  everi movi come truli make impact joaquin perf...\n",
              "2                    Only certain people can relate\\n  ...  thi movi felt alon isol truli relat you unders...\n",
              "3                          Perfect in every aspect.\\n  ...  truli masterpiec the best hollywood film one b...\n",
              "4                                  The Hype is real\\n  ...  most time movi anticip like end fall short way...\n",
              "..                                                ...  ...                                                ...\n",
              "95                          Astonishing Masterpiece\\n  ...  what incred ride i almost motionless throughou...\n",
              "96                                    Nonsense plot\\n  ...  arthur fleck live invalid mother he suffer men...\n",
              "97             I feel like everyone is brain washed\\n  ...  i thought movi complet utter garbag i dont und...\n",
              "98   I made an account just to rate my disappoint ...  ...  i expect masterpiec oscar worthi film hype i a...\n",
              "99                   Stop comparing it with Endgame\\n  ...  joker liter achiev cinemat histori there doubt...\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab_type": "code",
        "outputId": "4c73a6e5-729c-4f70-f34c-a0acec52dd96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "from nltk import ngrams,FreqDist,bigrams\n",
        "import pandas as panda\n",
        "import string,io,os\n",
        "from google.colab import drive,files\n",
        "\n",
        "def calculate_n_grams(data,n):\n",
        "  n_grams = []\n",
        "  for each in data['Preprocessed Reviews'][:10]:\n",
        "    n_grams.append(ngrams(each.split(), n))\n",
        "  return n_grams\n",
        "\n",
        "def get_reviewData():\n",
        "  data = panda.read_csv('UserReview.csv')\n",
        "  return data\n",
        "\n",
        "def freq_cal(n_grams):\n",
        "  freq_ngram = []\n",
        "  for each in n_grams:\n",
        "    fdist = nltk.FreqDist(each)\n",
        "    freq_ngram.append(list(fdist.most_common()))\n",
        "  return freq_ngram\n",
        "\n",
        "def freq(n_grams):\n",
        "  freq_ngram = []\n",
        "  for each in n_grams:\n",
        "    fdist = nltk.FreqDist(each)\n",
        "    freq_ngram.append(fdist)\n",
        "  return freq_ngram\n",
        "\n",
        "'''datafr = get_reviewData()\n",
        "trigrams = calculate_n_grams(datafr,3)\n",
        "print(\"\\nQuestion 1: Count all Tri-grams:\\n\")\n",
        "print(\"Tri-gram\\t\\tCount\")\n",
        "for each in freq_cal(trigrams):\n",
        "  for gram in each:\n",
        "    print(str(gram[0])+\"\\t\"+str(gram[1]))'''\n",
        "\n",
        "\n",
        "def count_prob():\n",
        "  data = get_reviewData()['Preprocessed Reviews'][:10]\n",
        "  uni_grams = [word_tokenize(each) for each in data]\n",
        "  bi_grams = [list(ngrams(each,2)) for each in uni_grams]\n",
        "  uni_frq = freq_cal(uni_grams)\n",
        "  bi_freq = freq_cal(bi_grams)\n",
        " \n",
        "  count_prob_bi = []\n",
        "\n",
        "  for i in range(len(bi_freq)):\n",
        "    prob_each = []\n",
        "    for each in bi_freq[i]:\n",
        "      c_b = int(each[1])\n",
        "      w2 = each[0][0]\n",
        "      for e in uni_frq[i]:\n",
        "        if e[0]==w2:\n",
        "          c_w2 = int(e[1])\n",
        "      prob_each.append(c_b/c_w2)\n",
        "    count_prob_bi.append(prob_each)\n",
        "  return count_prob_bi\n",
        "\n",
        "list_prob = count_prob()\n",
        "\n",
        "for each in list_prob:\n",
        "  print(each)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('i', 'person'), 1), (('person', 'saw'), 1), (('saw', 'hype'), 1), (('hype', 'claim'), 1), (('claim', 'masterpiec'), 1), (('masterpiec', 'overreact'), 1), (('overreact', 'overblown'), 1), (('overblown', 'excit'), 1), (('excit', 'anoth'), 1), (('anoth', 'joker'), 1), (('joker', 'base'), 1), (('base', 'film'), 1), (('film', 'i'), 1), (('i', 'thought'), 1), (('thought', 'look'), 1), (('look', 'solid'), 1), (('solid', 'best'), 1), (('best', 'even'), 1), (('even', 'bit'), 1), (('bit', 'pretenti'), 1), (('pretenti', 'trailer'), 1), (('trailer', 'say'), 1), (('say', 'i'), 1), (('i', 'incred'), 1), (('incred', 'wrong'), 1), (('wrong', 'thi'), 1), (('thi', 'massiv'), 1), (('massiv', 'achiev'), 1), (('achiev', 'cinema'), 1), (('cinema', 'that'), 1), (('that', 'extrem'), 1), (('extrem', 'rare'), 1), (('rare', 'day'), 1), (('day', 'age'), 1), (('age', 'cgi'), 1), (('cgi', 'nonsens'), 1), (('nonsens', 'reboot'), 1), (('reboot', 'while'), 1), (('while', 'somewhat'), 1), (('somewhat', 'reboot'), 1), (('reboot', 'sort'), 1), (('sort', 'standalon'), 1), (('standalon', 'origin'), 1), (('origin', 'tale'), 1), (('tale', 'impecc'), 1), (('impecc', 'start'), 1), (('start', 'finish'), 1), (('finish', 'echo'), 1), (('echo', 'resembl'), 1), (('resembl', 'best'), 1), (('best', 'joker'), 1), (('joker', 'origin'), 1), (('origin', 'comic'), 1), (('comic', 'past'), 1), (('past', 'joaquin'), 1), (('joaquin', 'bleed'), 1), (('bleed', 'sweat'), 1), (('sweat', 'cri'), 1), (('cri', 'everi'), 1), (('everi', 'drop'), 1), (('drop', 'magnific'), 1), (('magnific', 'dedic'), 1), (('dedic', 'perform'), 1), (('perform', 'heath'), 1), (('heath', 'ledger'), 1), (('ledger', 'would'), 1), (('would', 'proud'), 1), (('proud', 'thi'), 1), (('thi', 'undoubtedli'), 1), (('undoubtedli', 'greatest'), 1), (('greatest', 'act'), 1), (('act', 'perform'), 1), (('perform', 'sinc'), 1), (('sinc', 'heath'), 1), (('heath', 'joker'), 1), (('joker', 'the'), 1), (('the', 'direct'), 1), (('direct', 'write'), 1), (('write', 'slickli'), 1), (('slickli', 'brilliant'), 1), (('brilliant', 'bleak'), 1), (('bleak', 'set'), 1), (('set', 'tone'), 1), (('tone', 'palpabl'), 1), (('palpabl', 'throughout'), 1), (('throughout', 'when'), 1), (('when', 'film'), 1), (('film', 'place'), 1), (('place', 'blown'), 1), (('blown', 'away'), 1), (('away', 'everi'), 1), (('everi', 'audienc'), 1), (('audienc', 'member'), 1), (('member', 'awestruck'), 1), (('awestruck', 'wit'), 1), (('wit', 'film'), 1), (('film', 'could'), 1), (('could', 'still'), 1), (('still', 'transport'), 1), (('transport', 'charact'), 1), (('charact', 'world'), 1), (('world', 'exist'), 1), (('exist', 'believ'), 1), (('believ', 'hype'), 1), (('hype', 'thi'), 1), (('thi', 'go'), 1), (('go', 'rever'), 1), (('rever', 'transcend'), 1), (('transcend', 'masterpiec'), 1), (('masterpiec', 'cinema'), 1)]ERROR! Session/line number was not unique in \n",
            "database. History logging moved to new session[(('everi', 'movi'), 1), (('movi', 'come'), 1), (('come', 'truli'), 1), (('truli', 'make'), 1), (('make', 'impact'), 1), (('impact', 'joaquin'), 1), (('joaquin', 'perform'), 1), (('perform', 'scenographi'), 1), (('scenographi', 'brillianc'), 1), (('brillianc', 'grotesqu'), 1), (('grotesqu', 'haunt'), 1), (('haunt', 'cringi'), 1), (('cringi', 'hard'), 1), (('hard', 'watch'), 1), (('watch', 'time'), 1), (('time', 'mesmer'), 1), (('mesmer', 'wont'), 1), (('wont', 'blink'), 1), (('blink', 'eye'), 1), (('eye', 'watch'), 1), (('watch', 'tragic'), 1), (('tragic', 'serious'), 1), (('serious', 'funni'), 1), (('funni', 'moment'), 1), (('moment', 'emot'), 1), (('emot', 'rollercoast'), 1), (('rollercoast', 'sometim'), 1), (('sometim', 'multipl'), 1), (('multipl', 'emot'), 1), (('emot', 'poppingup'), 1), (('poppingup', 'timethi'), 1), (('timethi', 'far'), 1), (('far', 'typic'), 1), (('typic', 'actionriddl'), 1), (('actionriddl', 'predict'), 1), (('predict', 'superhero'), 1), (('superhero', 'movi'), 1), (('movi', 'proper'), 1), (('proper', 'psycholog'), 1), (('psycholog', 'thrillerdrama'), 1), (('thrillerdrama', 'singl'), 1), (('singl', 'best'), 1), (('best', 'charact'), 1), (('charact', 'develop'), 1), (('develop', 'i'), 1), (('i', 'ever'), 1), (('ever', 'seen'), 1)] 68\n",
            "\n",
            "[(('make', 'feel'), 2), (('thi', 'movi'), 1), (('movi', 'felt'), 1), (('felt', 'alon'), 1), (('alon', 'isol'), 1), (('isol', 'truli'), 1), (('truli', 'relat'), 1), (('relat', 'you'), 1), (('you', 'understand'), 1), (('understand', 'motiv'), 1), (('motiv', 'feel'), 1), (('feel', 'sorri'), 1), (('sorri', 'charact'), 1), (('charact', 'a'), 1), (('a', 'lot'), 1), (('lot', 'peopl'), 1), (('peopl', 'see'), 1), (('see', 'movi'), 1), (('movi', 'think'), 1), (('think', 'encourag'), 1), (('encourag', 'violenc'), 1), (('violenc', 'but'), 1), (('but', 'truli'), 1), (('truli', 'movi'), 1), (('movi', 'encourag'), 1), (('encourag', 'everi'), 1), (('everi', 'one'), 1), (('one', 'u'), 1), (('u', 'becom'), 1), (('becom', 'better'), 1), (('better', 'person'), 1), (('person', 'treat'), 1), (('treat', 'everyon'), 1), (('everyon', 'respect'), 1), (('respect', 'make'), 1), (('feel', 'like'), 1), (('like', 'belong'), 1), (('belong', 'world'), 1), (('world', 'instead'), 1), (('instead', 'make'), 1), (('feel', 'isol'), 1)]\n",
            "[(('best', 'film'), 2), (('truli', 'masterpiec'), 1), (('masterpiec', 'the'), 1), (('the', 'best'), 1), (('best', 'hollywood'), 1), (('hollywood', 'film'), 1), (('film', 'one'), 1), (('one', 'best'), 1), (('film', 'decad'), 1), (('decad', 'and'), 1), (('and', 'truli'), 1), (('truli', 'best'), 1), (('film', 'bring'), 1), (('bring', 'comic'), 1), (('comic', 'book'), 1), (('book', 'chillingli'), 1), (('chillingli', 'realist'), 1), (('realist', 'real'), 1), (('real', 'ife'), 1), (('ife', 'remark'), 1), (('remark', 'direct'), 1), (('direct', 'cinematographi'), 1), (('cinematographi', 'music'), 1), (('music', 'act'), 1), (('act', 'some'), 1), (('some', 'peopl'), 1), (('peopl', 'surpris'), 1), (('surpris', 'find'), 1), (('find', 'disturb'), 1), (('disturb', 'violent'), 1), (('violent', 'necess'), 1), (('necess', 'messag'), 1), (('messag', 'it'), 1), (('it', 'societi'), 1), (('societi', 'reflect'), 1), (('reflect', 'underappreciatedunrecognizedbulli'), 1), (('underappreciatedunrecognizedbulli', 'peopl'), 1), (('peopl', 'prove'), 1), (('prove', 'someth'), 1), (('someth', 'the'), 1), (('the', 'way'), 1), (('way', 'show'), 1), (('show', 'class'), 1), (('class', 'differ'), 1), (('differ', 'corrupt'), 1), (('corrupt', 'rich'), 1), (('rich', 'talent'), 1), (('talent', 'rule'), 1), (('rule', 'other'), 1), (('other', 'around'), 1), (('around', 'exagger'), 1), (('exagger', 'that'), 1), (('that', 'make'), 1), (('make', 'differ'), 1), (('differ', 'it'), 1), (('it', 'believ'), 1), (('believ', 'there'), 1), (('there', 'could'), 1), (('could', 'multipl'), 1), (('multipl', 'joker'), 1), (('joker', 'live'), 1), (('live', 'societi'), 1), (('societi', 'could'), 1), (('could', 'shake'), 1), (('shake', 'around'), 1), (('around', 'much'), 1), (('much', 'bitter'), 1), (('bitter', 'way'), 1), (('way', 'film'), 1), (('film', 'show'), 1), (('show', 'make'), 1), (('make', 'peopl'), 1), (('peopl', 'uncomfort'), 1), (('uncomfort', 'peopl'), 1), (('peopl', 'consid'), 1), (('consid', 'wake'), 1), (('wake', 'call'), 1), (('call', 'messag'), 1), (('messag', 'first'), 1), (('first', 'film'), 1), (('film', 'a'), 1), (('a', 'perfect'), 1), (('perfect', 'film'), 1)]\n",
            "[(('most', 'time'), 1), (('time', 'movi'), 1), (('movi', 'anticip'), 1), (('anticip', 'like'), 1), (('like', 'end'), 1), (('end', 'fall'), 1), (('fall', 'short'), 1), (('short', 'way'), 1), (('way', 'short'), 1), (('short', 'joker'), 1), (('joker', 'first'), 1), (('first', 'time'), 1), (('time', 'i'), 1), (('i', 'happi'), 1), (('happi', 'hype'), 1), (('hype', 'plea'), 1), (('plea', 'ignor'), 1), (('ignor', 'complaint'), 1), (('complaint', 'pernici'), 1), (('pernici', 'violenc'), 1), (('violenc', 'embarrass'), 1), (('embarrass', 'say'), 1), (('say', 'least'), 1), (('least', 'we'), 1), (('we', 'havent'), 1), (('havent', 'seen'), 1), (('seen', 'comic'), 1), (('comic', 'movi'), 1), (('movi', 'real'), 1), (('real', 'if'), 1), (('if', 'ever'), 1), (('ever', 'deserv'), 1), (('deserv', 'better'), 1), (('better', 'class'), 1), (('class', 'crimin'), 1), (('crimin', 'phillip'), 1), (('phillip', 'phoenix'), 1), (('phoenix', 'deliv'), 1), (('deliv', 'thi'), 1), (('thi', 'dark'), 1), (('dark', 'joker'), 1), (('joker', 'is'), 1), (('is', 'dark'), 1), (('dark', 'fall'), 1), (('fall', 'love'), 1), (('love', 'villain'), 1), (('villain', 'the'), 1), (('the', 'bad'), 1), (('bad', 'guy'), 1), (('guy', 'alway'), 1), (('alway', 'romant'), 1), (('romant', 'anyway'), 1)]\n",
            "[(('the', 'dark'), 2), (('dark', 'knight'), 2), (('joaquin', 'phoenix'), 1), (('phoenix', 'give'), 1), (('give', 'tour'), 1), (('tour', 'de'), 1), (('de', 'forc'), 1), (('forc', 'perform'), 1), (('perform', 'fearless'), 1), (('fearless', 'stun'), 1), (('stun', 'emot'), 1), (('emot', 'depth'), 1), (('depth', 'physic'), 1), (('physic', 'it'), 1), (('it', 'imposs'), 1), (('imposs', 'talk'), 1), (('talk', 'without'), 1), (('without', 'referenc'), 1), (('referenc', 'heath'), 1), (('heath', 'ledger'), 1), (('ledger', 'oscarwin'), 1), (('oscarwin', 'perform'), 1), (('perform', 'the'), 1), (('knight', 'wide'), 1), (('wide', 'consid'), 1), (('consid', 'definit'), 1), (('definit', 'liveact'), 1), (('liveact', 'portray'), 1), (('portray', 'joker'), 1), (('joker', 'let'), 1), (('let', 'talk'), 1), (('talk', 'the'), 1), (('the', 'fact'), 1), (('fact', 'everyon'), 1), (('everyon', 'go'), 1), (('go', 'stun'), 1), (('stun', 'phoenix'), 1), (('phoenix', 'accomplish'), 1), (('accomplish', 'mani'), 1), (('mani', 'thought'), 1), (('thought', 'imposs'), 1), (('imposs', 'portray'), 1), (('portray', 'match'), 1), (('match', 'potenti'), 1), (('potenti', 'exce'), 1), (('exce', 'the'), 1), (('knight', 'clown'), 1), (('clown', 'princ'), 1), (('princ', 'crime'), 1)]\n",
            "[(('let', 'start'), 1), (('start', 'say'), 1), (('say', 'joaquin'), 1), (('joaquin', 'phoneix'), 1), (('phoneix', 'doesnt'), 1), (('doesnt', 'get'), 1), (('get', 'oscar'), 1), (('oscar', 'movi'), 1), (('movi', 'then'), 1), (('then', 'oscar'), 1), (('oscar', 'cancel'), 1), (('cancel', 'phoneix'), 1), (('phoneix', 'amaz'), 1), (('amaz', 'mightv'), 1), (('mightv', 'heard'), 1), (('heard', 'everi'), 1), (('everi', 'review'), 1), (('review', 'ever'), 1), (('ever', 'but'), 1), (('but', 'todd'), 1), (('todd', 'phillip'), 1), (('phillip', 'is'), 1), (('is', 'best'), 1), (('best', 'the'), 1), (('the', 'stori'), 1), (('stori', 'line'), 1), (('line', 'take'), 1), (('take', 'visual'), 1), (('visual', 'breathtak'), 1), (('breathtak', 'the'), 1), (('the', 'score'), 1), (('score', 'omg'), 1), (('omg', 'score'), 1), (('score', 'everi'), 1), (('everi', 'time'), 1), (('time', 'score'), 1), (('score', 'came'), 1), (('came', 'i'), 1), (('i', 'felt'), 1), (('felt', 'uncomfort'), 1), (('uncomfort', 'like'), 1), (('like', 'someth'), 1), (('someth', 'horribl'), 1), (('horribl', 'happen'), 1), (('happen', 'it'), 1), (('it', 'great'), 1), (('great', 'the'), 1), (('the', 'inspir'), 1), (('inspir', 'taxi'), 1), (('taxi', 'driver'), 1), (('driver', 'king'), 1), (('king', 'comedi'), 1), (('comedi', 'add'), 1), (('add', 'much'), 1), (('much', 'movi'), 1), (('movi', 'and'), 1), (('and', 'i'), 1), (('i', 'got'), 1), (('got', 'honest'), 1), (('honest', 'there'), 1), (('there', 'scene'), 1), (('scene', 'violent'), 1), (('violent', 'and'), 1), (('and', 'disturb'), 1), (('disturb', 'but'), 1), (('but', 'i'), 1), (('i', 'honestli'), 1), (('honestli', 'expect'), 1), (('expect', 'wayyyi'), 1), (('wayyyi', 'violent'), 1), (('violent', 'controversi'), 1), (('controversi', 'go'), 1), (('go', 'overal'), 1), (('overal', 'movi'), 1), (('movi', 'great'), 1), (('great', 'come'), 1), (('come', 'oscar'), 1), (('oscar', 'season'), 1), (('season', 'need'), 1), (('need', 'nomin'), 1), (('nomin', 'best'), 1), (('best', 'pictur'), 1), (('pictur', 'screenplay'), 1), (('screenplay', 'cinematographi'), 1), (('cinematographi', 'actor'), 1), (('actor', 'score'), 1), (('score', 'director'), 1)]\n",
            "[(('i', 'get'), 1), (('get', 'peopl'), 1), (('peopl', 'hate'), 1), (('hate', 'it'), 1), (('it', 'polit'), 1), (('polit', 'messag'), 1), (('messag', 'peopl'), 1), (('peopl', 'think'), 1), (('think', 'need'), 1), (('need', 'get'), 1), (('get', 'empathi'), 1), (('empathi', 'arthur'), 1), (('arthur', 'mad'), 1), (('mad', 'but'), 1), (('but', 'come'), 1), (('come', 'point'), 1), (('point', 'never'), 1), (('never', 'enjoy'), 1), (('enjoy', 'masterpiec'), 1), (('masterpiec', 'joaquin'), 1), (('joaquin', 'phoenix'), 1), (('phoenix', 'todd'), 1), (('todd', 'phillip'), 1), (('phillip', 'overdid'), 1), (('overdid', 'movi'), 1), (('movi', 'the'), 1), (('the', 'actingmus'), 1), (('actingmus', 'cinematographi'), 1), (('cinematographi', 'amaz'), 1), (('amaz', 'plea'), 1), (('plea', 'enjoy'), 1), (('enjoy', 'movi'), 1), (('movi', 'without'), 1), (('without', 'overthink'), 1)]\n",
            "[(('i', 'seen'), 1), (('seen', 'joker'), 1), (('joker', 'yesterday'), 1), (('yesterday', 'venic'), 1), (('venic', 'earli'), 1), (('earli', 'illfat'), 1), (('illfat', 'screen'), 1), (('screen', 'we'), 1), (('we', 'troubl'), 1), (('troubl', 'audio'), 1), (('audio', 'lead'), 1), (('lead', 'nearhour'), 1), (('nearhour', 'delay'), 1), (('delay', 'definit'), 1), (('definit', 'worth'), 1), (('worth', 'itjok'), 1), (('itjok', 'deserv'), 1), (('deserv', 'present'), 1), (('present', 'venic'), 1), (('venic', 'film'), 1), (('film', 'festiv'), 1), (('festiv', 'event'), 1), (('event', 'regard'), 1), (('regard', 'cinema'), 1), (('cinema', 'form'), 1), (('form', 'art'), 1), (('art', 'film'), 1), (('film', 'far'), 1), (('far', 'blockbust'), 1), (('blockbust', 'mere'), 1), (('mere', 'entertain'), 1), (('entertain', 'movi'), 1), (('movi', 'film'), 1), (('film', 'genr'), 1), (('genr', 'areit'), 1), (('areit', 'focus'), 1), (('focus', 'psych'), 1), (('psych', 'main'), 1), (('main', 'charact'), 1), (('charact', 'slowli'), 1), (('slowli', 'crumbl'), 1), (('crumbl', 'pressur'), 1), (('pressur', 'societi'), 1), (('societi', 'and'), 1), (('and', 'thu'), 1), (('thu', 'joaquin'), 1), (('joaquin', 'phoenix'), 1), (('phoenix', 'wonder'), 1), (('wonder', 'perform'), 1), (('perform', 'earn'), 1), (('earn', 'almost'), 1), (('almost', 'sure'), 1), (('sure', 'nomin'), 1), (('nomin', 'oscar'), 1), (('oscar', 'least'), 1), (('least', 'it'), 1), (('it', 'take'), 1), (('take', 'joker'), 1), (('joker', 'differ'), 1), (('differ', 'ledger'), 1), (('ledger', 'id'), 1), (('id', 'say'), 1), (('say', 'equal'), 1), (('equal', 'good'), 1), (('good', 'the'), 1), (('the', 'main'), 1), (('main', 'differ'), 1), (('differ', 'might'), 1), (('might', 'ledger'), 1), (('ledger', 'joker'), 1), (('joker', 'ration'), 1), (('ration', 'act'), 1), (('act', 'insan'), 1), (('insan', 'phoenix'), 1), (('phoenix', 'insan'), 1), (('insan', 'rootdespit'), 1), (('rootdespit', 'movi'), 1), (('movi', 'superhero'), 1), (('superhero', 'villain'), 1), (('villain', 'joker'), 1), (('joker', 'much'), 1), (('much', 'superior'), 1), (('superior', 'movi'), 1), (('movi', 'genr'), 1), (('genr', 'id'), 1), (('id', 'exclud'), 1), (('exclud', 'dark'), 1), (('dark', 'knight'), 1), (('knight', 'trilog'), 1), (('trilog', 'joker'), 1), (('joker', 'easili'), 1), (('easili', 'good'), 1), (('good', 'nolan'), 1), (('nolan', 'movi'), 1), (('movi', 'least'), 1), (('least', 'close'), 1), (('close', 'it'), 1), (('it', 'smallscal'), 1), (('smallscal', 'film'), 1), (('film', 'distinct'), 1), (('distinct', 'style'), 1), (('style', 'cinematographi'), 1), (('cinematographi', 'can'), 1), (('can', 'not'), 1), (('not', 'appreci'), 1), (('appreci', 'set'), 1), (('set', 'cinephil'), 1), (('cinephil', 'refer'), 1), (('refer', 'howev'), 1), (('howev', 'feel'), 1), (('feel', 'forc'), 1), (('forc', 'overli'), 1), (('overli', 'opress'), 1), (('opress', 'notabl'), 1), (('notabl', 'similar'), 1), (('similar', 'scorses'), 1), (('scorses', 'taxi'), 1), (('taxi', 'driver'), 1), (('driver', 'the'), 1), (('the', 'king'), 1), (('king', 'comedi'), 1), (('comedi', 'also'), 1), (('also', 'chaplin'), 1), (('chaplin', 'modern'), 1), (('modern', 'time'), 1), (('time', 'somewhat'), 1), (('somewhat', 'referencedi'), 1), (('referencedi', 'eager'), 1), (('eager', 'see'), 1), (('see', 'noncomed'), 1), (('noncomed', 'effort'), 1), (('effort', 'todd'), 1), (('todd', 'phillip'), 1), (('phillip', 'thi'), 1), (('thi', 'movi'), 1), (('movi', 'far'), 1), (('far', 'probabl'), 1), (('probabl', 'best'), 1), (('best', 'worst'), 1), (('worst', 'contest'), 1), (('contest', 'far'), 1), (('far', 'dolor'), 1), (('dolor', 'y'), 1), (('y', 'gloria'), 1), (('gloria', 'onc'), 1), (('onc', 'upon'), 1), (('upon', 'time'), 1), (('time', 'hollywood'), 1), (('hollywood', 'convinc'), 1)]\n",
            "[(('movi', 'get'), 2), (('it', 'sad'), 1), (('sad', 'joaquin'), 1), (('joaquin', 'miss'), 1), (('miss', 'oscar'), 1), (('oscar', 'the'), 1), (('the', 'gladiat'), 1), (('gladiat', 'compel'), 1), (('compel', 'villain'), 1), (('villain', 'but'), 1), (('but', 'i'), 1), (('i', 'quit'), 1), (('quit', 'confid'), 1), (('confid', 'win'), 1), (('win', 'joker'), 1), (('joker', 'damn'), 1), (('damn', 'movi'), 1), (('movi', 'keep'), 1), (('keep', 'u'), 1), (('u', 'toe'), 1), (('toe', 'time'), 1), (('time', 'unpredict'), 1), (('unpredict', 'storylin'), 1), (('storylin', 'realli'), 1), (('realli', 'deep'), 1), (('deep', 'interest'), 1), (('interest', 'plot'), 1), (('plot', 'did'), 1), (('did', 'i'), 1), (('i', 'forget'), 1), (('forget', 'mention'), 1), (('mention', 'act'), 1), (('act', 'damn'), 1), (('damn', 'do'), 1), (('do', 'niro'), 1), (('niro', 'joaquin'), 1), (('joaquin', 'teach'), 1), (('teach', 'u'), 1), (('u', 'realli'), 1), (('realli', 'star'), 1), (('star', 'act'), 1), (('act', 'to'), 1), (('to', 'enjoy'), 1), (('enjoy', 'movi'), 1), (('get', 'wine'), 1), (('wine', 'hand'), 1), (('hand', 'close'), 1), (('close', 'curtain'), 1), (('curtain', 'turn'), 1), (('turn', 'ur'), 1), (('ur', 'cellphon'), 1), (('cellphon', 'put'), 1), (('put', 'disturb'), 1), (('disturb', 'sign'), 1), (('sign', 'ur'), 1), (('ur', 'door'), 1), (('door', 'best'), 1), (('best', 'dark'), 1), (('dark', 'thriller'), 1), (('thriller', 'suspens'), 1), (('suspens', 'movi'), 1), (('get', 'experi'), 1)]\n",
            "[0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5]\n",
            "[1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 0.5, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333]\n",
            "[0.6666666666666666, 0.5, 1.0, 0.5, 0.3333333333333333, 1.0, 0.16666666666666666, 1.0, 0.16666666666666666, 1.0, 1.0, 0.5, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 0.25, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.16666666666666666, 0.5, 0.5, 0.25, 1.0, 0.25, 1.0, 1.0, 1.0, 0.5, 1.0, 0.16666666666666666, 1.0, 1.0]\n",
            "[1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "[0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.25, 1.0, 0.25, 0.5, 1.0, 0.25, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.25]\n",
            "[1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0]\n",
            "[1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 1.0, 1.0, 0.2, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.2, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.2, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.2, 1.0, 1.0, 0.2, 1.0, 1.0, 0.2, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 0.2, 0.5, 1.0, 0.5, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]\n",
            "[0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}